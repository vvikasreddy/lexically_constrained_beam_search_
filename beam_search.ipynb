{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvikasreddy/lexically_constrained_beam_search_/blob/main/beam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "Marian MT model : https://huggingface.co/docs/transformers/model_doc/marian\n",
        "\n",
        "Code to get the logits : https://huggingface.co/docs/transformers/main_classes/output\n",
        "\n",
        "to get the BOS and EOS tokens: https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig.decoder_start_token_id\n",
        "\n",
        "get topk values : https://pytorch.org/docs/stable/generated/torch.topk.html\n",
        "\n",
        "ideas and core implementation drawn from this paper: https://arxiv.org/pdf/1704.07138\n",
        "\n",
        "bleu score : https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
        "\n",
        "rogue score : https://huggingface.co/spaces/evaluate-metric/rouge/blob/main/README.md\n",
        "\n",
        "(not directly related to beam search project)\n",
        "\n",
        "\n",
        "reference to link google colab with .py file from git  : https://colab.research.google.com/github/jckantor/cbe61622/blob/master/docs/A.02-Downloading_Python_source_files_from_github.ipynb\n",
        "\n",
        "reference to get rogue score working : https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "\n"
      ],
      "metadata": {
        "id": "irykjFilkb0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading essential modules"
      ],
      "metadata": {
        "id": "4FVX-zdj1zoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "  return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# reference to get this working : https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working"
      ],
      "metadata": {
        "id": "ayaas3MZvTpk",
        "outputId": "53d88bfb-493b-40fe-9a8c-08c93b56312e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTeRXGcN1aLG",
        "outputId": "71017e70-277f-409c-ab9f-5acbe2699118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "39v0-NQ5jW6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "BsIWSdWT1nXy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the dataset, considering the wmt turkish - english translation"
      ],
      "metadata": {
        "id": "4ryAXcurjhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"wmt/wmt16\", \"tr-en\")"
      ],
      "metadata": {
        "id": "wh8kntBaDQkQ",
        "outputId": "a7a07859-0b20-4276-abe2-3b5c131b3c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glancing the organization of the dataset"
      ],
      "metadata": {
        "id": "xUKJaLMBjwHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZmmV6aM18Zm",
        "outputId": "6ad6a6c6-e5cb-4603-84a4-66b96f04ad73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 205756\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1001\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VskMyKZb7W3U",
        "outputId": "ae468a72-c1c5-4e46-8629-06fd35c77bee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': \"Kosovo's privatisation process is under scrutiny\",\n",
              "  'tr': \"Kosova'nın özelleştirme süreci büyüteç altında\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the tokenizer and model, based of Marian-NMT"
      ],
      "metadata": {
        "id": "xKqvu8eej9_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-D25X8-3fDH",
        "outputId": "fa297110-82d9-4b72-c1ab-e4291d14431d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq8hERj4pMq",
        "outputId": "00efac20-b919-4f19-c54b-73bbf214766c",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(62389, 512, padding_idx=62388)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLU()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLU()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=62389, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a look at the number of beams used by the model.\n",
        "model.config.num_beams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kypz225xV07O",
        "outputId": "4b2ad72c-049f-418b-f4b3-4b2ef85efe24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing the turkish and english translation\n",
        "print(ds[\"validation\"][1][\"translation\"][\"tr\"])\n",
        "print(ds[\"validation\"][1][\"translation\"][\"en\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld36ZcknI28I",
        "outputId": "79282543-6fa1-4933-d180-a05e2e332bfe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norveç'in beş milyon insanı en yüksek yaşam standartlarının tadını çıkarıyor, sadece Avrupa'da değil, dünyada.\n",
            "Norway's five million people enjoy one of the highest standards of living, not just in Europe, but in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting and importing the constraints from my github"
      ],
      "metadata": {
        "id": "716cOsrattrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to import constraints and store in a local directory, from my git. (code reference cicted above.)\n",
        "\n",
        "user = \"vvikasreddy\"\n",
        "repo = \"lexically_constrained_beam_search_\"\n",
        "pyfile = \"constraints.py\"\n",
        "\n",
        "# i.e url is \"https://github.com/vvikasreddy/lexically_constrained_beam_search_/blob/main/constraints.py\"\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{pyfile}\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "\n",
        "import constraints"
      ],
      "metadata": {
        "id": "jrClS2MolMXW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f92cc6-a69c-4c58-fc17-c485200fff48"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 01:22:36--  https://raw.githubusercontent.com/vvikasreddy/lexically_constrained_beam_search_/main/constraints.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4914 (4.8K) [text/plain]\n",
            "Saving to: ‘constraints.py’\n",
            "\n",
            "constraints.py      100%[===================>]   4.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-07 01:22:36 (67.7 MB/s) - ‘constraints.py’ saved [4914/4914]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# takes almost 4 minutes to get the constraints, you will see 3 progress bars\n",
        "c = constraints.get_constraints()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L9LeDVKbZlK1",
        "outputId": "407997ac-faa6-4cc5-b284-a491a15cf09b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205756/205756 [00:44<00:00, 4671.14it/s]\n",
            "100%|██████████| 205756/205756 [01:06<00:00, 3105.42it/s]\n",
            "100%|██████████| 26221852/26221852 [00:41<00:00, 634381.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"some of the constraints are :\")\n",
        "\n",
        "# Extract 5 random keys\n",
        "random_keys = random.sample(list(c.keys()), 5)\n",
        "\n",
        "for key in random_keys:\n",
        "  print(key, c[key])\n",
        "\n",
        "print(\"The length of the constraints is\", len(c))"
      ],
      "metadata": {
        "id": "z-aHWm6_h8wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3bb1395-237a-47d2-e98f-aa49e307a8e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "some of the constraints are :\n",
            "('Ekim', 'Pazartesi') (('Monday', '(October'), 0.9722479175671673)\n",
            "('ve', 'NATO') (('and', 'NATO'), 1.0560217638878333)\n",
            "('ve', 'Sırbistan') (('and', 'Serbia'), 0.9671508806281317)\n",
            "('en', 'yüksek') (('the', 'highest'), 1.0629455738333067)\n",
            "('Reuters,', 'BBC,') (('Reuters,', 'BBC,'), 0.9497800176080862)\n",
            "The length of the constraints is 570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions for Beam Search"
      ],
      "metadata": {
        "id": "iCaHlsGX2vQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(src, n = 2, ):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    src: text for which ngrams should be returned\n",
        "    n : represents the value of ngrams\n",
        "\n",
        "  returns:\n",
        "    returns ngrams\n",
        "  \"\"\"\n",
        "\n",
        "  src = src.split(\" \")\n",
        "  src = [tuple(src[i:i+n]) for i in range(len(src) - n + 1)]\n",
        "  return src\n",
        "\n",
        "def constraints_tokens(src, c):\n",
        "  \"\"\"\n",
        "  args:\n",
        "    src: It is the turkish sentence, to which we want to return the constraints\n",
        "    c: represents the entire list of constraints\n",
        "\n",
        "  returns:\n",
        "    returns the corresponding constraints of the src text\n",
        "  \"\"\"\n",
        "\n",
        "  # gets the ngrams\n",
        "  ngrams = get_ngrams(src)\n",
        "\n",
        "  constraints_src = []\n",
        "  for ngram in ngrams:\n",
        "\n",
        "    # if ngram is present then add it to the constraints list\n",
        "    if ngram in c:\n",
        "      f = c[ngram][0]\n",
        "      for gram in f:\n",
        "        if  gram in constraints_src: continue\n",
        "        out = tokenizer(gram, return_tensors=\"pt\")\n",
        "        constraints_src.append(out[\"input_ids\"])\n",
        "\n",
        "  return constraints_src\n",
        "\n",
        "# this are some of the example constraints of the sample text in turkish\n",
        "constraints_tokens(\"Southeast European Times için Priştine'den Muhamet Brayşori'nin haberi -- 21/03/12\", c)"
      ],
      "metadata": {
        "id": "m31ZnR46R1S0",
        "outputId": "b513dcc1-d212-43f4-c624-515c7e2ab06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[3113,   56,   47, 1517,    0]]),\n",
              " tensor([[5827, 1786,  373,    0]]),\n",
              " tensor([[5827, 1786,  373,    0]]),\n",
              " tensor([[3762,    0]]),\n",
              " tensor([[27,  0]]),\n",
              " tensor([[3113,   56,   47, 1517,    0]]),\n",
              " tensor([[21,  0]]),\n",
              " tensor([[ 4388, 10158,   204,     0]]),\n",
              " tensor([[1417,    0]]),\n",
              " tensor([[6041,   47, 2628,    0]]),\n",
              " tensor([[27,  0]]),\n",
              " tensor([[3113,   56,   47, 1517,    0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(decoder_input):\n",
        "  \"\"\"\n",
        "  code used for sanity, to visualize the constraints.\n",
        "  args:\n",
        "    decoder_input : takes in decoder token ids\n",
        "  returns:\n",
        "    returns the tokens\"\"\"\n",
        "\n",
        "  return tokenizer.decode(decoder_input.squeeze(), skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "ecRWZg9IDyQl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_indices(ds, c):\n",
        "  \"\"\"\n",
        "  Since wmt dataset is not domain speicific, I am trying to only take the sentences, for which constraints are present.\n",
        "\n",
        "  args:\n",
        "    ds: represents the hugging face WMT dataset\n",
        "    c: represents the constraints\n",
        "  returns:\n",
        "    indices: which contains the first 1000 indices, that have constraints present.\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  indices = []\n",
        "  count = 100\n",
        "  i = 0\n",
        "\n",
        "  while count:\n",
        "    x = ds[\"train\"][i][\"translation\"][\"tr\"]\n",
        "    if constraints_tokens(x, c):\n",
        "      count -=1\n",
        "      indices.append(i)\n",
        "    i+=1\n",
        "\n",
        "  return indices\n",
        "\n",
        "print(\"The last index in all of the 3000 is,\", get_indices(ds,c)[-1])"
      ],
      "metadata": {
        "id": "dYfl26o1HxrL",
        "outputId": "caa4e49a-d5d7-44f4-8e1a-da831cce7ba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The last index in all of the 3000 is, 273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Results(Dataset):\n",
        "\n",
        "  \"\"\"\n",
        "  Results class is the implementation of the Beam search\n",
        "\n",
        "  Methods:\n",
        "\n",
        "  generate_translation: Generates the next best token\n",
        "  get_the_text: returns the translated text at the end of beam search\n",
        "  get_top_k_prob: given a bunch of translations, it returns the top k beams\n",
        "  constraints_tokens: returns the constraints of the particular target sentence (i.e in our case turkish sentence)\n",
        "  beam_search: tries to return the best translation, we have a cap of maxlength 30, and beamsize of 6\n",
        "  __len__: returns the length of indices considered (in our case 1000, same as the size of validation set)\n",
        "  __getitem__: for each item called returns the translated sentence at the end of beam search\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, ds, c, indices, device = None):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          ds : represents the Hugging face dataset\n",
        "          c: represents all the constraints generated\n",
        "          indices: these are the indices which we consider for the test\n",
        "          device: chooses the device based on the configs\n",
        "      \"\"\"\n",
        "      self.data = ds\n",
        "      self.c = c\n",
        "      self.indices = indices\n",
        "      if device is None:\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def generate_translation(self, src_text, decoder_input=[], probabilities=[], get_constrained_token_probability=-1, k=6):\n",
        "\n",
        "    \"\"\"\n",
        "    this method generates the k best translations and their probabilites\n",
        "\n",
        "    args:\n",
        "      src_text : target sentence in turkish\n",
        "      decoder_inputs : holds the token ids in english, up till that timestep\n",
        "      probabliities : holds the corresponding probabilities of the decoder_inputs\n",
        "      get_constrained_token_probablity: holds value -1, if there is no constraint, else contains the constraint token id\n",
        "      k : represents the beam size, which is set to 6\n",
        "\n",
        "    returns:\n",
        "      decoder_input_ids: it holds the decoder input ids so far\n",
        "      probabilities: holds the corresponding probabilities of the decoder inputs\n",
        "      vis_data: used for sanity purposes, to visualize the generated data\n",
        "      \"\"\"\n",
        "\n",
        "    # holds cuda if available else cpu\n",
        "    device = self.device\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize input\n",
        "    encoder_inputs = tokenizer(src_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # If decoder_input is empty, include the decoder start token\n",
        "    if len(decoder_input) == 0:\n",
        "      # Initial decoder start token has probability 1\n",
        "      probabilities = torch.tensor([[1.0]]).to(device)\n",
        "      decoder_input = torch.tensor([[model.config.decoder_start_token_id]]).to(device)\n",
        "    else:\n",
        "      # Ensure decoder_input and probabilities are on the correct device\n",
        "      decoder_input = decoder_input.to(device)\n",
        "      if probabilities != [] :\n",
        "        probabilities = probabilities.to(device)\n",
        "\n",
        "    # Change the model to eval mode and stop the computation of gradients\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      # Generate tokens\n",
        "      outputs = model(\n",
        "          input_ids=encoder_inputs.input_ids,\n",
        "          attention_mask=encoder_inputs.attention_mask,\n",
        "          decoder_input_ids=decoder_input\n",
        "      )\n",
        "\n",
        "      # Get the most frequently generated token\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "      # Constraint handling\n",
        "      if get_constrained_token_probability != -1:\n",
        "          softmax_ = torch.softmax(next_token_logits, dim=-1)\n",
        "          return softmax_[0][get_constrained_token_probability]\n",
        "\n",
        "      # Get the top k tokens with maximum logits value\n",
        "      top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k=k)\n",
        "\n",
        "    # Initializing output containers\n",
        "    decoder_input_tokens = []\n",
        "    probs = []\n",
        "    vis_data = []\n",
        "\n",
        "    for indx, id in enumerate(top_indices[0]):\n",
        "        # Concatenate new tokens and probabilities\n",
        "        new_decoder_input = torch.cat([decoder_input, id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        new_probs = torch.cat([probabilities, top_probs[0][indx].unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        decoder_input_tokens.append(new_decoder_input)\n",
        "        probs.append(new_probs)\n",
        "\n",
        "        # Generate visualization data, used for sanity (removed for submission)\n",
        "        vis_data.append((vis_data, tokenizer.decode(new_decoder_input.squeeze(), skip_special_tokens=True)))\n",
        "\n",
        "    return decoder_input_tokens, probs, vis_data\n",
        "\n",
        "  def get_the_text(self, x):\n",
        "    \"\"\"\n",
        "    Given decoder token ids, it converts to tokens\n",
        "\n",
        "    args:\n",
        "      x: represents the decoder token ids\n",
        "    returns:\n",
        "      the corresponding translated sentences in english\n",
        "    \"\"\"\n",
        "\n",
        "    token_id_list = x\n",
        "\n",
        "    # moves tensors to cpu\n",
        "    token_ids_batch = [tensor_item.cpu().squeeze().tolist() for tensor_item in token_id_list]\n",
        "\n",
        "    decoded_sentences = tokenizer.batch_decode(token_ids_batch, skip_special_tokens=True)\n",
        "\n",
        "    if not decoded_sentences: return [\"\"]\n",
        "    else: return decoded_sentences\n",
        "\n",
        "  def get_top_k_prob(self, A, B, k=6):\n",
        "\n",
        "    \"\"\"\n",
        "    takes in decoder inputs, and corresponding probablities and returns the k best beams\n",
        "\n",
        "    args:\n",
        "      A: represents the list of decoder input ids\n",
        "      B: represents the list of correspondding probabilities\n",
        "      k : it is the beam size\n",
        "    returns:\n",
        "      returns the k best values.\"\"\"\n",
        "\n",
        "    # container to hold the cummulative probablility sum of the beams\n",
        "    d = {}\n",
        "\n",
        "    # cummulative sum\n",
        "    for indx, val in enumerate(B):\n",
        "      cum_sum = torch.prod(val)\n",
        "      d[cum_sum] = indx\n",
        "\n",
        "    # sort to the cummulative probability sum\n",
        "    sorted_keys = sorted(d.keys(), reverse = True)\n",
        "\n",
        "    # containers to store top k values\n",
        "    top_k_indices = []\n",
        "    top_k_sequences = []\n",
        "\n",
        "    for key in sorted_keys[:k]:\n",
        "      top_k_indices.append(A[d[key]])\n",
        "      top_k_sequences.append(B[d[key]])\n",
        "\n",
        "    return top_k_sequences, top_k_indices\n",
        "\n",
        "  def constraints_tokens(self, src):\n",
        "    \"\"\"\n",
        "    args:\n",
        "      src: It is the turkish sentence, to which we want to return the constraints\n",
        "      c: represents the entire list of constraints\n",
        "\n",
        "    returns:\n",
        "      returns the corresponding constraints of the src text\n",
        "  \"\"\"\n",
        "    # gets the ngrams\n",
        "    ngrams = get_ngrams(src)\n",
        "    constraints_src = []\n",
        "\n",
        "    for ngram in ngrams:\n",
        "      # if ngram is present then add it to the constraints list\n",
        "      if ngram in self.c:\n",
        "        f = self.c[ngram][0]\n",
        "        for gram in f:\n",
        "\n",
        "          if  gram in constraints_src: continue\n",
        "          out = tokenizer(gram, return_tensors=\"pt\")\n",
        "          constraints_src.append(out[\"input_ids\"])\n",
        "\n",
        "    return constraints_src\n",
        "\n",
        "  def beam_search(self, maxlen, numC, k, src, constrained_tokens):\n",
        "\n",
        "    \"\"\"\n",
        "    Combines all the methods to return the k best beams\n",
        "\n",
        "    args:\n",
        "      maxlen: it is the max length, until where we generate sentences\n",
        "      numC: represents the number of constraints\n",
        "      k: represents the number of beams\n",
        "      src: represents the target sentences, in turkish language\n",
        "      constrained_tokens: represnts the constraints associated with the corresponding sentence\n",
        "\n",
        "    returns:\n",
        "      returns 2 sentences,\n",
        "      1. sentence containing the constraints associated with the sentence\n",
        "      2. sentence generated without any constraints\n",
        "    \"\"\"\n",
        "\n",
        "    # device where values are present\n",
        "    device = self.device\n",
        "\n",
        "    decoder_start_token = model.config.decoder_start_token_id\n",
        "\n",
        "    # initialize the grids\n",
        "    grids = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "    probs_grid  = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "\n",
        "    # intialize the first grid to start hyp\n",
        "    grids[0][0] = [1]\n",
        "\n",
        "    generated_constraint_index = 0\n",
        "\n",
        "    # iterate through the timestep\n",
        "    for t in range(1, maxlen):\n",
        "\n",
        "        index_c = max(0, (numC - t) - maxlen)\n",
        "        # iterate through constraints\n",
        "        for c in range(index_c, min(t, numC) + 1):\n",
        "\n",
        "            # Prepare batched generation to reduce individual calls, g holds the decoder input tokens, for the current timestep.\n",
        "            g = []\n",
        "\n",
        "            # storing decoder inputs\n",
        "            decoder_inputs = []\n",
        "            probs = []\n",
        "            vis_data = []\n",
        "\n",
        "            # generation of translations for current hypotheses\n",
        "            for indx, element in enumerate(grids[t-1][c]):\n",
        "\n",
        "              if type(element) == int:\n",
        "                decoder_input = []\n",
        "                prev_probs =[]\n",
        "              else:\n",
        "                decoder_input = element.to(device)\n",
        "                prev_probs = probs_grid[t-1][c][indx].to(device)\n",
        "\n",
        "              # collecting the batch of translations\n",
        "              t_g, t_probs, t_vis_data = self.generate_translation(src_text=src, decoder_input=decoder_input, probabilities=prev_probs)\n",
        "\n",
        "              # adding to the current lists\n",
        "              g.extend(t_g)\n",
        "              probs.extend(t_probs)\n",
        "              vis_data.extend(t_vis_data)\n",
        "\n",
        "            # retrieve the probability of the constraint and add that to the decoder_input.\n",
        "            if c > 0 and constrained_tokens:\n",
        "\n",
        "              for indx, element in enumerate(grids[t-1][c-1]):\n",
        "\n",
        "                if c == 1 and t == 1:\n",
        "                  decoder_inputs = torch.tensor([[model.config.decoder_start_token_id]]).to(device)\n",
        "                  prob = torch.tensor([[1]]).to(device)\n",
        "                else:\n",
        "                  decoder_inputs = element.to(device)\n",
        "                  prob = probs_grid[t-1][c-1][indx].to(device)\n",
        "\n",
        "                # iterating, because a constraint can be made up of many token ids\n",
        "                partial_constraints = constrained_tokens[c - 1].tolist()\n",
        "\n",
        "                # iterating, because a constraint can be made up of many token ids\n",
        "                for partial_constraint in partial_constraints[0]:\n",
        "                  if partial_constraint == 0: continue\n",
        "\n",
        "\n",
        "                  cons = self.generate_translation(src, decoder_input=decoder_input, get_constrained_token_probability=partial_constraint)\n",
        "                  decoder_inputs = torch.cat([decoder_inputs, torch.tensor(partial_constraint).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n",
        "\n",
        "                  # check to ensure the constraints are generated, else a warning is raised\n",
        "                  if cons is None:  # Check if the constraints list is empty\n",
        "                      warnings.warn(\"Generated constraints are empty. Proceeding without constraints.\", UserWarning)\n",
        "\n",
        "                  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n",
        "\n",
        "                  # appending to generated token ids to(g), and probabilites to probs\n",
        "                  g.append(decoder_inputs)\n",
        "                  probs.append(prob)\n",
        "\n",
        "            # storing the k best token ids to the current grid.\n",
        "            probs_grid[t][c], grids[t][c] = self.get_top_k_prob(g, probs, k)\n",
        "\n",
        "    # return the best token ids which contains all the constraints, and ones without any constraints.\n",
        "    return self.get_top_k_prob(grids[maxlen -1][numC], probs_grid[maxlen - 1][numC], k = 1), self.get_top_k_prob(grids[maxlen -1][0], probs_grid[maxlen - 1][0], k = 1)\n",
        "\n",
        "  def __len__(self):\n",
        "      \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "      return len(self.indices)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Retrieves a single target sentence, and returns prediction with constraints, prediction without any constraints, and reference translation\n",
        "\n",
        "      args:\n",
        "          idx: Index of the sample to retrieve.\n",
        "\n",
        "      Returns:\n",
        "          returns target sentence, prediction with constraints, prediction without any constraints, and reference translation\n",
        "      \"\"\"\n",
        "\n",
        "      sample = self.data[\"train\"][self.indices[idx]][\"translation\"][\"tr\"]\n",
        "\n",
        "      # gets the constraints for that sentence\n",
        "      constraints = self.constraints_tokens(sample)\n",
        "\n",
        "      # using beam search gets the predicted translations, both with and without constraints\n",
        "      prediction, prediction_without_constraints = self.beam_search(maxlen= 23, numC=len(constraints), k = 4, src = sample, constrained_tokens = constraints)\n",
        "\n",
        "      # convert the token ids to tokens\n",
        "      prediction = self.get_the_text(prediction[1])\n",
        "      prediction_without_constraints = self.get_the_text(prediction_without_constraints[1])\n",
        "      result = self.data[\"train\"][self.indices[idx]][\"translation\"][\"en\"]\n",
        "\n",
        "      # if len(prediction) > 1: prediction = prediction[0][:]\n",
        "      # else: prediction = []\n",
        "      # if len(prediction_without_constraints) > 0 : prediction_without_constraints = prediction_without_constraints[0][:]\n",
        "\n",
        "      return sample, prediction[0], prediction_without_constraints[0], result"
      ],
      "metadata": {
        "id": "iZSxRCdiBmA_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the instance of the Results class\n",
        "indices = get_indices(ds,c)\n",
        "results = Results(ds,c, indices = indices)\n",
        "\n",
        "# generating sentences for 43th index\n",
        "results[43]"
      ],
      "metadata": {
        "id": "GkErwbFeFpwO",
        "outputId": "0b1becca-0add-4984-ad7b-9566db0536b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-72cf223229c8>:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Sırbistan'ın, Kosova'da yaşayan Sırp nüfusun ve yanı sıra dünya genelinde birçok ülkenin bu eyleme karşı çıkması, ciddi sorunlarla karşı karşıya kalacağımızın habercisi.\",\n",
              " \"Serbia's opposition to this action bloc, the Serb population in Koso as well as many\",\n",
              " \"Serbia's opposition to the Serbian population in Kosovo, as well as many countries around the world\",\n",
              " 'The fact that neither Serbia nor the Serbian population in Kosovo as well as a number of countries throughout the world agree with such an act speaks of a period of serious challenges we are facing.')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store predictions and actual texts\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  predictions = []\n",
        "  predictions_without_constraints = []\n",
        "  references = []\n",
        "\n",
        "  # using a dataloader, to get the batches of 15\n",
        "  dataloader = DataLoader(results, batch_size=2, shuffle=False)\n",
        "\n",
        "  # Iterate through dataloader and storing results in containers\n",
        "  for tr_text, pred_text, pred_cons_text, actual_text in tqdm(dataloader):\n",
        "\n",
        "    predictions.extend([ref.split() for ref in pred_text])\n",
        "    predictions_without_constraints.extend([ref.split() for ref in pred_cons_text])\n",
        "    references.extend([[ref.split()] for ref in actual_text])\n"
      ],
      "metadata": {
        "id": "feByO35xG0c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0387867c-6c0b-4a01-ae12-e331c5b3ec4d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-20-72cf223229c8>:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n",
            "100%|██████████| 50/50 [35:24<00:00, 42.49s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "\n",
        "def BLEU_score(references, translations):\n",
        "  \"\"\"\n",
        "  Averages the BLEU score over all the translations\n",
        "\n",
        "  args:\n",
        "    references: represents acutal translations\n",
        "    translations: represents predicted translations\n",
        "  returns:\n",
        "    average BLEU score\n",
        "  \"\"\"\n",
        "  bleu_scores = []\n",
        "\n",
        "  # since my constraints were generated only for ngrams of size 2\n",
        "  weights = [0.5, 0.5,0,0]\n",
        "\n",
        "  for ref, trans in zip(references, translations):\n",
        "    # Calculate BLEU score for this sentence\n",
        "    score = sentence_bleu(ref, trans, weights = weights)\n",
        "    bleu_scores.append(score)\n",
        "\n",
        "  # Calculate and return average BLEU score\n",
        "  return sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "\n",
        "bleu_score_constraints = BLEU_score(references, predictions)\n",
        "bleu_score_without_constraints = BLEU_score(references, predictions_without_constraints)\n",
        "print(f\"BLEU Score with constraints: {bleu_score_constraints:.4f}, BLEU score without constraints: {bleu_score_without_constraints:.4f}\")"
      ],
      "metadata": {
        "id": "qTg5A2VhHGLC",
        "outputId": "777dbfc1-9757-4be6-bd4e-7fb3bddc18fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score with constraints: 0.2515, BLEU score without constraints: 0.4039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "Tx9sbK52V5Nz",
        "outputId": "64cc3cfe-fa5d-4a01-f2be-d6e4b3452847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "Eh-qzYCt3i_q",
        "outputId": "5c0f612b-415d-44ff-e380-fad38105789e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=f0dcd58ab342d4fc5111017129888a4ed3fa4d4fd12e0128f2dcf1bf31d492f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def calculate_rouge_with_evaluate(references, translations):\n",
        "    \"\"\"\n",
        "    Calculate average ROUGE-1, ROUGE-2, and ROUGE-L F1 scores using the `evaluate` library.\n",
        "\n",
        "    args:\n",
        "      references: represents acutal translations\n",
        "      translations: represents predicted translations\n",
        "    returns:\n",
        "      average BLEU score\n",
        "  \"\"\"\n",
        "    # Load the ROUGE metric\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "    # joining the references and translations into one list\n",
        "    references_flat = [\" \".join(ref[0]) for ref in references]\n",
        "    translations_flat = [\" \".join(trans) for trans in translations]\n",
        "    results = rouge.compute(predictions=translations_flat, references=references_flat)\n",
        "\n",
        "    return {\n",
        "        \"rouge_1\": results[\"rouge1\"],\n",
        "        \"rouge_2\": results[\"rouge2\"],\n",
        "        \"rouge_L\": results[\"rougeL\"]\n",
        "    }\n",
        "\n",
        "# Calculate ROUGE scores for predictions with and without constraints\n",
        "rouge_scores_constraints = calculate_rouge_with_evaluate(references, predictions)\n",
        "rouge_scores_no_constraints = calculate_rouge_with_evaluate(references, predictions_without_constraints)\n",
        "\n",
        "print(f\"ROUGE-1 with constraints: {rouge_scores_constraints['rouge_1']:.4f}\")\n",
        "print(f\"ROUGE-2 with constraints: {rouge_scores_constraints['rouge_2']:.4f}\")\n",
        "print(f\"ROUGE-L with constraints: {rouge_scores_constraints['rouge_L']:.4f}\")\n",
        "\n",
        "print(f\"ROUGE-1 without constraints: {rouge_scores_no_constraints['rouge_1']:.4f}\")\n",
        "print(f\"ROUGE-2 without constraints: {rouge_scores_no_constraints['rouge_2']:.4f}\")\n",
        "print(f\"ROUGE-L without constraints: {rouge_scores_no_constraints['rouge_L']:.4f}\")\n"
      ],
      "metadata": {
        "id": "n3xXKVeCPjY7",
        "outputId": "10cf872d-b769-4070-8061-7c0f8b5b2de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1 with constraints: 0.4838\n",
            "ROUGE-2 with constraints: 0.2797\n",
            "ROUGE-L with constraints: 0.4195\n",
            "ROUGE-1 without constraints: 0.6283\n",
            "ROUGE-2 without constraints: 0.4563\n",
            "ROUGE-L without constraints: 0.5792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eaD2_g7BV3jt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
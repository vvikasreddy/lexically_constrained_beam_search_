{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvikasreddy/lexically_constrained_beam_search_/blob/main/beam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "Marian MT model : https://huggingface.co/docs/transformers/model_doc/marian\n",
        "\n",
        "Code to get the logits : https://huggingface.co/docs/transformers/main_classes/output\n",
        "\n",
        "to get the BOS and EOS tokens: https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig.decoder_start_token_id\n",
        "\n",
        "get topk values : https://pytorch.org/docs/stable/generated/torch.topk.html\n",
        "\n",
        "ideas and core implementation drawn from this paper: https://arxiv.org/pdf/1704.07138\n",
        "\n",
        "reference to link google colab with .py file from git : https://colab.research.google.com/github/jckantor/cbe61622/blob/master/docs/A.02-Downloading_Python_source_files_from_github.ipynb\n"
      ],
      "metadata": {
        "id": "irykjFilkb0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading essential modules"
      ],
      "metadata": {
        "id": "4FVX-zdj1zoV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "rTeRXGcN1aLG",
        "outputId": "0a44f407-e8e3-4c4f-d4c7-1fe885d0b263"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-c998cfbab00f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "39v0-NQ5jW6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "BsIWSdWT1nXy"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the dataset, considering the wmt turkish - english translation"
      ],
      "metadata": {
        "id": "4ryAXcurjhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"wmt/wmt16\", \"tr-en\")"
      ],
      "metadata": {
        "id": "wh8kntBaDQkQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glancing the organization of the dataset"
      ],
      "metadata": {
        "id": "xUKJaLMBjwHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZmmV6aM18Zm",
        "outputId": "01004eb5-4735-4db6-f7e7-8bd44c99b506"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 205756\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1001\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VskMyKZb7W3U",
        "outputId": "15f2508e-829d-4dd0-e030-c9dd068ef0c6"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': \"Kosovo's privatisation process is under scrutiny\",\n",
              "  'tr': \"Kosova'nın özelleştirme süreci büyüteç altında\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the tokenizer and model, based of Marian-NMT"
      ],
      "metadata": {
        "id": "xKqvu8eej9_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-D25X8-3fDH",
        "outputId": "87485f5c-d15f-484d-8033-ed10648ccb7d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq8hERj4pMq",
        "outputId": "41619694-b2f2-456b-8b05-1f916cd9a59b",
        "collapsed": true
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(62389, 512, padding_idx=62388)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLU()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLU()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=62389, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.num_beams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kypz225xV07O",
        "outputId": "8a0fafa3-a7f0-4b21-bd1e-8af76ee2febe"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(generate_translation(ds['validation'][1]['translation']['tr']))"
      ],
      "metadata": {
        "id": "XUR-FySHjfHZ",
        "collapsed": true
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"validation\"][1][\"translation\"][\"tr\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ld36ZcknI28I",
        "outputId": "adf5f797-0ef2-4337-c0bc-a10888f3f8a3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Norveç'in beş milyon insanı en yüksek yaşam standartlarının tadını çıkarıyor, sadece Avrupa'da değil, dünyada.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"validation\"][1][\"translation\"][\"en\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "2qleapnBJH_O",
        "outputId": "468c78d2-6acb-46dc-e560-e0f9916816b1"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Norway's five million people enjoy one of the highest standards of living, not just in Europe, but in the world.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting the constraints"
      ],
      "metadata": {
        "id": "716cOsrattrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to import constraints and store in a local directory, from my git\n",
        "\n",
        "user = \"vvikasreddy\"\n",
        "repo = \"lexically_constrained_beam_search_\"\n",
        "pyfile = \"constraints.py\"\n",
        "\n",
        "# i.e url is \"https://github.com/vvikasreddy/lexically_constrained_beam_search_/blob/main/constraints.py\"\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{pyfile}\"\n",
        "# !wget --no-cache --backups=1 {url}\n",
        "\n",
        "import constraints"
      ],
      "metadata": {
        "id": "jrClS2MolMXW",
        "collapsed": true
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes almost 4 minutes to get the constraints, you will see 3 progress bars\n",
        "c = constraints.get_constraints()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L9LeDVKbZlK1",
        "outputId": "088b2c98-b62f-4a46-ca37-f31c57f3e7b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205756/205756 [00:44<00:00, 4673.58it/s]\n",
            "100%|██████████| 205756/205756 [01:04<00:00, 3177.54it/s]\n",
            "100%|██████████| 26221852/26221852 [00:42<00:00, 623351.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"some of the constraints are :\")\n",
        "\n",
        "# Extract 5 random keys\n",
        "random_keys = random.sample(list(c.keys()), 5)\n",
        "\n",
        "for key in random_keys:\n",
        "  print(key, c[key])\n",
        "\n",
        "print(\"The length of the constraints is\", len(c))"
      ],
      "metadata": {
        "id": "z-aHWm6_h8wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_translation(src_text, decoder_input = [], probabilites = [], get_constrained_token_probability = -1, k = 5):\n",
        "\n",
        "  \"\"\"\n",
        "    returns decoder_input_tokens, probs, vis_data\n",
        "\n",
        "    decoder_input_tokens : next top k tokens, or constraints probability if get_constrained_token_probabliity != -1\n",
        "    probs : corresponding probablities of decoder_input_tokens\n",
        "    vis_data : top k beams\n",
        "\n",
        "    generate the decoder input ids and corresponding probabilities\n",
        "    src_text : It is the source text\n",
        "    decoder_input : Represents the decoder tokens\n",
        "    probabilities : Represents corresponding decoder token probablities\n",
        "    get_constrained_token_probabliity : holds the constraint, -1 indicates no constraint,\n",
        "    k : number of beams to be generated, default is 5\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  decoder_input_tokens = []\n",
        "  probs = []\n",
        "  vis_data = []\n",
        "\n",
        "  # Tokenize input\n",
        "  encoder_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
        "\n",
        "  # if decoder_input is empty, then include the decoder start token\n",
        "  if decoder_input == []:\n",
        "    # intial decoder start token has probability 1\n",
        "    probabilites = torch.tensor([[1]])\n",
        "    decoder_input = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "  # change the model to eval mode and stop the computation of gradients.\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids=encoder_inputs.input_ids,\n",
        "        attention_mask=encoder_inputs.attention_mask,\n",
        "        decoder_input_ids=decoder_input\n",
        "    )\n",
        "\n",
        "    # gets the most frequenlty generated token.\n",
        "    next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    # constraint, if provided, returns the probability.\n",
        "    if get_constrained_token_probability != -1:\n",
        "      softmax_  = torch.softmax(next_token_logits, dim=-1)\n",
        "      return softmax_[0][get_constrained_token_probability]\n",
        "\n",
        "    # get the top k tokens with maximum logits value\n",
        "    top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k = k)\n",
        "\n",
        "  for indx, id in enumerate(top_indices[0]):\n",
        "    decoder_input_tokens.append(torch.cat([decoder_input, id.unsqueeze(0).unsqueeze(0)], dim=1))\n",
        "    probs.append(torch.cat([probabilites, top_probs[0][indx].unsqueeze(0).unsqueeze(0)], dim=1))\n",
        "    vis_data.append((vis_data, tokenizer.decode(decoder_input_tokens[indx].squeeze(), skip_special_tokens = True)))\n",
        "\n",
        "  return decoder_input_tokens, probs, vis_data\n",
        "\n",
        "x,y,z = generate_translation(ds['validation'][1]['translation']['tr'], decoder_input = torch.tensor([[62388,  1969]]), probabilites = torch.tensor([[0.0000, 0.0000]]))\n",
        "print( x)\n",
        "print(y)\n",
        "print(z)\n",
        "# c"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Lb1AIoB-akSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Returns decoder_input_tokens, probs, vis_data\n",
        "\n",
        "    Args:\n",
        "        src_text (str): The source text to translate\n",
        "        decoder_input (torch.Tensor, optional): Decoder tokens. Defaults to [].\n",
        "        probabilities (torch.Tensor, optional): Corresponding decoder token probabilities. Defaults to [].\n",
        "        get_constrained_token_probability (int, optional): Constraint token index. Defaults to -1.\n",
        "        k (int, optional): Number of beams to generate. Defaults to 5.\n",
        "        device (torch.device, optional): Device to run the model on. Defaults to None (auto-detect).\n",
        "\n",
        "    Returns:\n",
        "        tuple: decoder_input_tokens, probs, vis_data\n",
        "    \"\"\"\n",
        "  "
      ],
      "metadata": {
        "id": "3hiqs57vT0kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_translation(src_text, decoder_input=[], probabilities=[], get_constrained_token_probability=-1, k=5, device=None):\n",
        "  # print(\"he\")\n",
        "  # Auto-detect device if not specified\n",
        "  if device is None:\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Move model to the specified device\n",
        "  model.to(device)\n",
        "\n",
        "    # Tokenize input\n",
        "  encoder_inputs = tokenizer(src_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # If decoder_input is empty, include the decoder start token\n",
        "  if len(decoder_input) == 0:\n",
        "\n",
        "    # Initial decoder start token has probability 1\n",
        "    probabilities = torch.tensor([[1.0]]).to(device)\n",
        "    decoder_input = torch.tensor([[model.config.decoder_start_token_id]]).to(device)\n",
        "  else:\n",
        "    # Ensure decoder_input and probabilities are on the correct device\n",
        "    # print(probabilities, \"am in else\")\n",
        "    # print(decoder_input, \"am in else\")\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    if probabilities != [] :\n",
        "      probabilities = probabilities.to(device)\n",
        "\n",
        "  # Change the model to eval mode and stop the computation of gradients\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      # Generate tokens\n",
        "    outputs = model(\n",
        "        input_ids=encoder_inputs.input_ids,\n",
        "        attention_mask=encoder_inputs.attention_mask,\n",
        "        decoder_input_ids=decoder_input\n",
        "    )\n",
        "\n",
        "  # Get the most frequently generated token\n",
        "    next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    # Constraint handling\n",
        "    if get_constrained_token_probability != -1:\n",
        "        softmax_ = torch.softmax(next_token_logits, dim=-1)\n",
        "        return softmax_[0][get_constrained_token_probability]\n",
        "\n",
        "    # Get the top k tokens with maximum logits value\n",
        "    top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k=k)\n",
        "\n",
        "  # Prepare output containers\n",
        "  decoder_input_tokens = []\n",
        "  probs = []\n",
        "  vis_data = []\n",
        "\n",
        "  for indx, id in enumerate(top_indices[0]):\n",
        "      # Concatenate new tokens and probabilities\n",
        "      new_decoder_input = torch.cat([decoder_input, id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "      new_probs = torch.cat([probabilities, top_probs[0][indx].unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "\n",
        "      decoder_input_tokens.append(new_decoder_input)\n",
        "      probs.append(new_probs)\n",
        "\n",
        "      # Generate visualization data\n",
        "      vis_data.append((vis_data, tokenizer.decode(new_decoder_input.squeeze(), skip_special_tokens=True)))\n",
        "\n",
        "  return decoder_input_tokens, probs, vis_data\n",
        "\n",
        "# Example usage\n",
        "# Assuming 'ds', 'model', and 'tokenizer' are defined\n",
        "x, y, z = generate_translation(\n",
        "    ds['validation'][1]['translation']['tr'],\n",
        "    decoder_input=torch.tensor([[62388, 1969]]),\n",
        "    probabilities=torch.tensor([[0.0000, 0.0000]])\n",
        ")\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "metadata": {
        "id": "ZWIXFq_Q2kVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(src, n = 2, ):\n",
        "\n",
        "  src = src.split(\" \")\n",
        "  src = [tuple(src[i:i+n]) for i in range(len(src) - n + 1)]\n",
        "\n",
        "  return src\n",
        "\n",
        "def constraints_tokens(src, c):\n",
        "  ngrams = get_ngrams(src)\n",
        "  constraints_src = []\n",
        "  for ngram in ngrams:\n",
        "    # print(ngram)\n",
        "    if ngram in c:\n",
        "      f = c[ngram][0]\n",
        "      for gram in f:\n",
        "\n",
        "        if  gram in constraints_src: continue\n",
        "        out = tokenizer(gram, return_tensors=\"pt\")\n",
        "        constraints_src.append(out[\"input_ids\"])\n",
        "        # print(constraints_src)\n",
        "  return constraints_src\n",
        "\n",
        "constraints_tokens(\"Southeast European Times için Priştine'den Muhamet Brayşori'nin haberi -- 21/03/12\", c)"
      ],
      "metadata": {
        "id": "m31ZnR46R1S0",
        "outputId": "07c8561e-b683-4087-af32-b02086365ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[3113,   56,   47, 1517,    0]]),\n",
              " tensor([[5827, 1786,  373,    0]]),\n",
              " tensor([[5827, 1786,  373,    0]]),\n",
              " tensor([[3762,    0]]),\n",
              " tensor([[27,  0]]),\n",
              " tensor([[3113,   56,   47, 1517,    0]]),\n",
              " tensor([[21,  0]]),\n",
              " tensor([[ 4388, 10158,   204,     0]]),\n",
              " tensor([[1417,    0]]),\n",
              " tensor([[6041,   47, 2628,    0]]),\n",
              " tensor([[27,  0]]),\n",
              " tensor([[3113,   56,   47, 1517,    0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = constraints_tokens(ds['validation'][1]['translation']['tr'] + ' Times' +  ' için', c)\n",
        "# # c\n",
        "# print(x)\n",
        "# print(get_input_ids(x))"
      ],
      "metadata": {
        "id": "YAE_CMBYSeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(decoder_input):\n",
        "  return tokenizer.decode(decoder_input.squeeze(), skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "ecRWZg9IDyQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_top_k_prob(A, B, k=2):\n",
        "\n",
        "  d = {}\n",
        "  # cummulative sum\n",
        "  for indx, val in enumerate(B):\n",
        "\n",
        "    cum_sum = torch.prod(val)\n",
        "    d[cum_sum] = indx\n",
        "\n",
        "  sorted_keys = sorted(d.keys(), reverse = True)\n",
        "\n",
        "  top_k_indices = []\n",
        "  top_k_sequences = []\n",
        "\n",
        "  for key in sorted_keys[:k]:\n",
        "    top_k_indices.append(A[d[key]])\n",
        "    top_k_sequences.append(B[d[key]])\n",
        "\n",
        "  return top_k_sequences, top_k_indices\n",
        "\n",
        "# sanity\n",
        "k = 2\n",
        "\n",
        "# A = [torch.tensor([[62388,   626,    13]]), torch.tensor([[62388,   626,     9]]), torch.tensor([[62388,   626,  1341]]), torch.tensor([[62388,   626,    27]])]\n",
        "# B = [torch.tensor([[1.0000, 0.0038, 0.2500]]), torch.tensor([[1.0000, 0.0038, 0.0619]]), torch.tensor([[1.0000, 0.0038, 0.0474]]), torch.tensor([[1.0000, 0.0038, 0.0425]])]\n",
        "\n",
        "\n",
        "A = [torch.tensor([[62388,  1969]]), torch.tensor([[62388,   323]]), torch.tensor([[62388,    67]]), torch.tensor([[62388,  1132]]), torch.tensor([[62388,   626]])]\n",
        "B = [torch.tensor([[1.0000, 0.8746]]), torch.tensor([[1.0000, 0.0156]]), torch.tensor([[1.0000, 0.0114]]), torch.tensor([[1.0000, 0.0039]]), torch.tensor([[1.0000, 0.0038]])]\n",
        "\n",
        "top_sequences, indices = get_top_k_prob(A, B, k)\n",
        "print(f\"Top {k} sequences:\", top_sequences)\n",
        "print(\"Their indices:\", indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGCXp6x4OEj2",
        "outputId": "28f3cfc5-b9a3-42b6-e9ea-6a481af89174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 2 sequences: [tensor([[1.0000, 0.8746]]), tensor([[1.0000, 0.0156]])]\n",
            "Their indices: [tensor([[62388,  1969]]), tensor([[62388,   323]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming x[1] is a list of tensors\n",
        "def get_the_text(x):\n",
        "  token_id_list = x  # Extract the list of tensors\n",
        "\n",
        "  # Move all tensors to CPU and convert to a list of lists\n",
        "  token_ids_batch = [tensor_item.cpu().squeeze().tolist() for tensor_item in token_id_list]\n",
        "\n",
        "  # Batch decode token IDs into sentences\n",
        "  decoded_sentences = tokenizer.batch_decode(token_ids_batch, skip_special_tokens=True)\n",
        "\n",
        "  return decoded_sentences\n",
        "\n",
        "get_the_text(x[1])"
      ],
      "metadata": {
        "id": "jGXjuwBb89vp",
        "outputId": "1b6f4e61-9c68-4e82-c681-5562f1e95cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kost']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since wmt dataset is not domain speicific, I am trying to only take the sentences, which have constraints defined.\n",
        "\n",
        "count = 0\n",
        "indices = []\n",
        "count = 3000\n",
        "i = 0\n",
        "\n",
        "while count:\n",
        "  x = ds[\"train\"][i][\"translation\"][\"tr\"]\n",
        "  if constraints_tokens(x, c):\n",
        "    count -=1\n",
        "    indices.append(i)\n",
        "  i+=1\n",
        "\n",
        "print(\"The last index in all of the 3000 is,\", indices[-1])"
      ],
      "metadata": {
        "id": "dYfl26o1HxrL",
        "outputId": "b0b58940-92e9-4062-fd4b-467f033d9397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The last index in all of the 3000 is, 6304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ds[\"train\"])\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Results(Dataset):\n",
        "\n",
        "  def __init__(self, ds, c, indices):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          data : represents the turkish sentences to be passed\n",
        "      \"\"\"\n",
        "      self.data = ds\n",
        "      self.c = c\n",
        "      self.indices = indices\n",
        "\n",
        "  def generate_translation(self, src_text, decoder_input=[], probabilities=[], get_constrained_token_probability=-1, k=5, device=None):\n",
        "    # print(\"he\")\n",
        "    # Auto-detect device if not specified\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move model to the specified device\n",
        "    model.to(device)\n",
        "\n",
        "      # Tokenize input\n",
        "    encoder_inputs = tokenizer(src_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # If decoder_input is empty, include the decoder start token\n",
        "    if len(decoder_input) == 0:\n",
        "\n",
        "      # Initial decoder start token has probability 1\n",
        "      probabilities = torch.tensor([[1.0]]).to(device)\n",
        "      decoder_input = torch.tensor([[model.config.decoder_start_token_id]]).to(device)\n",
        "    else:\n",
        "      # Ensure decoder_input and probabilities are on the correct device\n",
        "      # print(probabilities, \"am in else\")\n",
        "      # print(decoder_input, \"am in else\")\n",
        "      decoder_input = decoder_input.to(device)\n",
        "      if probabilities != [] :\n",
        "        probabilities = probabilities.to(device)\n",
        "\n",
        "    # Change the model to eval mode and stop the computation of gradients\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Generate tokens\n",
        "      outputs = model(\n",
        "          input_ids=encoder_inputs.input_ids,\n",
        "          attention_mask=encoder_inputs.attention_mask,\n",
        "          decoder_input_ids=decoder_input\n",
        "      )\n",
        "\n",
        "    # Get the most frequently generated token\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "      # Constraint handling\n",
        "      if get_constrained_token_probability != -1:\n",
        "          softmax_ = torch.softmax(next_token_logits, dim=-1)\n",
        "          return softmax_[0][get_constrained_token_probability]\n",
        "\n",
        "      # Get the top k tokens with maximum logits value\n",
        "      top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k=k)\n",
        "\n",
        "    # Prepare output containers\n",
        "    decoder_input_tokens = []\n",
        "    probs = []\n",
        "    vis_data = []\n",
        "\n",
        "    for indx, id in enumerate(top_indices[0]):\n",
        "        # Concatenate new tokens and probabilities\n",
        "        new_decoder_input = torch.cat([decoder_input, id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        new_probs = torch.cat([probabilities, top_probs[0][indx].unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "\n",
        "        decoder_input_tokens.append(new_decoder_input)\n",
        "        probs.append(new_probs)\n",
        "\n",
        "        # Generate visualization data\n",
        "        vis_data.append((vis_data, tokenizer.decode(new_decoder_input.squeeze(), skip_special_tokens=True)))\n",
        "\n",
        "    return decoder_input_tokens, probs, vis_data\n",
        "\n",
        "  def get_the_text(self, x):\n",
        "    token_id_list = x  # Extract the list of tensors\n",
        "\n",
        "    # Move all tensors to CPU and convert to a list of lists\n",
        "    token_ids_batch = [tensor_item.cpu().squeeze().tolist() for tensor_item in token_id_list]\n",
        "\n",
        "    # Batch decode token IDs into sentences\n",
        "    decoded_sentences = tokenizer.batch_decode(token_ids_batch, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_sentences\n",
        "  def get_top_k_prob(self, A, B, k=2):\n",
        "    d = {}\n",
        "    # cummulative sum\n",
        "    for indx, val in enumerate(B):\n",
        "\n",
        "      cum_sum = torch.prod(val)\n",
        "      d[cum_sum] = indx\n",
        "\n",
        "    sorted_keys = sorted(d.keys(), reverse = True)\n",
        "\n",
        "    top_k_indices = []\n",
        "    top_k_sequences = []\n",
        "\n",
        "    for key in sorted_keys[:k]:\n",
        "      top_k_indices.append(A[d[key]])\n",
        "      top_k_sequences.append(B[d[key]])\n",
        "\n",
        "    return top_k_sequences, top_k_indices\n",
        "\n",
        "  def constraints_tokens(self, src):\n",
        "    ngrams = get_ngrams(src)\n",
        "    constraints_src = []\n",
        "    for ngram in ngrams:\n",
        "      # print(ngram)\n",
        "      if ngram in self.c:\n",
        "        f = self.c[ngram][0]\n",
        "        for gram in f:\n",
        "\n",
        "          if  gram in constraints_src: continue\n",
        "          out = tokenizer(gram, return_tensors=\"pt\")\n",
        "          constraints_src.append(out[\"input_ids\"])\n",
        "          # print(constraints_src)\n",
        "    return constraints_src\n",
        "\n",
        "  def beam_search(self, maxlen, numC, k, src, constrained_tokens):\n",
        "\n",
        "    decoder_start_token = model.config.decoder_start_token_id\n",
        "\n",
        "    # initialize the grids\n",
        "    grids = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "    probs_grid  = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "\n",
        "    # intialize the first grid to start hyp\n",
        "    grids[0][0] = [1]\n",
        "\n",
        "    # remove during testsrc\n",
        "    # constrained_tokens = get_input_ids(constraints_tokens(, constraints))\n",
        "    # temporary\n",
        "    # constrained_tokens = constraints_tokens(\"Southeast European Times için Priştine'den Muhamet Brayşori'nin haberi -- 21/03/12\", constraints)\n",
        "\n",
        "    generated_constraint_index = 0\n",
        "\n",
        "    for t in range(1, maxlen):\n",
        "\n",
        "        index_c = max(0, (numC - t) - maxlen)\n",
        "\n",
        "        for c in range(index_c, min(t, numC) + 1):\n",
        "\n",
        "            # Prepare batched generation to reduce individual calls\n",
        "            s = []\n",
        "            g = []\n",
        "\n",
        "            # storing decoder inputs\n",
        "            decoder_inputs = []\n",
        "            probs = []\n",
        "            vis_data = []\n",
        "\n",
        "            # Batch generation of translations for current hypotheses\n",
        "            for indx, element in enumerate(grids[t-1][c]):\n",
        "\n",
        "              # guess there is no need for conditioning, just generate.\n",
        "              if type(element) == int:\n",
        "                decoder_input = []\n",
        "                prev_probs =[]\n",
        "              else:\n",
        "                decoder_input = element.cuda()  # Move to GPU\n",
        "                prev_probs = probs_grid[t-1][c][indx].cuda()  # Move to GPU\n",
        "\n",
        "              # Batch collection of translations\n",
        "              t_g, t_probs, t_vis_data = self.generate_translation(src_text=src, decoder_input=decoder_input, probabilities=prev_probs)\n",
        "\n",
        "              # Extend lists more efficiently\n",
        "              g.extend(t_g)\n",
        "              probs.extend(t_probs)\n",
        "              vis_data.extend(t_vis_data)\n",
        "\n",
        "            # retrieve the probability of the constraint and add that to the decoder_input.\n",
        "            if c > 0 and constrained_tokens:\n",
        "\n",
        "              for indx, element in enumerate(grids[t-1][c-1]):\n",
        "\n",
        "                if c == 1 and t == 1:\n",
        "                  decoder_inputs = torch.tensor([[model.config.decoder_start_token_id]]).cuda()  # Move to GPU\n",
        "                  prob = torch.tensor([[1]]).cuda()  # Move to GPU\n",
        "                else:\n",
        "                  decoder_inputs = element.cuda()  # Move to GPU\n",
        "                  prob = probs_grid[t-1][c-1][indx].cuda()  # Move to GPU\n",
        "\n",
        "                # print(constrained_tokens[c - 1])\n",
        "\n",
        "                # iterating, because a constraint can be made up of many token ids\n",
        "                partial_constraints = constrained_tokens[c - 1].tolist()\n",
        "                # print(partial_constraints, \"gandu\")\n",
        "                for partial_constraint in partial_constraints[0]:\n",
        "                  if partial_constraint == 0: continue\n",
        "                  # print(partial_constraint, \"tryint to dissolve\")\n",
        "                  cons = self.generate_translation(src, decoder_input=decoder_input, get_constrained_token_probability=partial_constraint)\n",
        "                  # print(cons, \"cons\", decoder_inputs, torch.tensor(partial_constraint).unsqueeze(0).unsqueeze(0))\n",
        "                  # Concatenate constraints\n",
        "                  decoder_inputs = torch.cat([decoder_inputs, torch.tensor(partial_constraint).unsqueeze(0).unsqueeze(0).cuda()], dim=1)\n",
        "                  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).cuda()], dim=1)\n",
        "\n",
        "                  g.append(decoder_inputs)\n",
        "                  probs.append(prob)\n",
        "\n",
        "            # Select top-k hypotheses\n",
        "            probs_grid[t][c], grids[t][c] = get_top_k_prob(g, probs, k)\n",
        "\n",
        "    return get_top_k_prob(grids[maxlen - 1][numC], probs_grid[maxlen - 1][numC], k = 1), get_top_k_prob(grids[maxlen -1][0], probs_grid[maxlen - 1][0], k = 1)\n",
        "\n",
        "  def __len__(self):\n",
        "      \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "      return len(self.indices)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Retrieve a single sample and its label.\n",
        "\n",
        "      Args:\n",
        "          idx (int): Index of the sample to retrieve.\n",
        "\n",
        "      Returns:\n",
        "          tuple: (sample, label) where sample is the data and label is the corresponding label.\n",
        "      \"\"\"\n",
        "      sample = self.data[\"test\"][self.indices[idx]][\"translation\"][\"tr\"]\n",
        "      constraints = self.constraints_tokens(sample)\n",
        "\n",
        "      prediction, prediction_without_constraints = self.beam_search(maxlen= 30, numC=len(constraints), k = 4, src = sample, constrained_tokens = constraints)\n",
        "      prediction = self.get_the_text(prediction[1])\n",
        "      prediction_without_constraints = self.get_the_text(prediction_without_constraints[1])\n",
        "      result = self.data[\"test\"][self.indices[idx]][\"translation\"][\"en\"]\n",
        "      return sample, prediction[0], prediction_without_constraints[0], result"
      ],
      "metadata": {
        "id": "iZSxRCdiBmA_"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = Results(ds,c, indices = indices)\n",
        "results[1]"
      ],
      "metadata": {
        "id": "GkErwbFeFpwO",
        "outputId": "3523c913-db31-4b43-81da-c4eb256cb636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-160-fc020ff109c0>:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).cuda()], dim=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Polisler evde Amy Prentiss\\'in cansız bedenini ve beyaz bir not defterine el yazısıyla yazılmış bir not buldu: \"Çok üzgünüm, keşke her şeyi geri döndürebilsem, Amy\\'yi çok sevmiştim ve o da bugüne kadar beni seven tek kadındı\" okunan notu Lamb\\'in imzaladığını belirtti memurlar.',\n",
              " 'so far, the police found Amy Prentiss\\' lifeless body and a note handwritten in a white notebook: \"I\\'m so',\n",
              " 'Police found Amy Prentiss\\' lifeless body in the house and a note handwritten in a white notebook: \"I\\'m so sorry',\n",
              " 'Inside the home, officers found Amy Prentiss\\' body and a hand-written note scribbled on a white legal pad: \"I am so very sorry I wish I could take it back I loved Amy and she is the only woman who ever loved me,\" read the letter authorities say was signed by Lamb.')"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Initialize lists to store predictions and actual texts\n",
        "predictions = []\n",
        "predictions_without_constraints = []\n",
        "references = []\n",
        "\n",
        "\n",
        "# Assuming 'results' is your dataset\n",
        "dataloader = DataLoader(results, batch_size=32, shuffle=True)\n",
        "\n",
        "# Iterate through the entire dataloader\n",
        "for tr_text, pred_text, pred_cons_text, actual_text in tqdm(dataloader):\n",
        "    # Extend the lists with the current batch\n",
        "    predictions.extend(pred_text)\n",
        "    predictions_without_constraints.extend(pred_cons_text)\n",
        "    # For BLEU score, we need references to be a list of lists of tokenized references\n",
        "    # Assuming actual_text is a list of strings\n",
        "    references.extend([[ref.split()] for ref in actual_text])\n"
      ],
      "metadata": {
        "id": "feByO35xG0c7",
        "outputId": "6310559e-2bc7-4af5-e07a-239e409e57fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/6430 [00:00<?, ?it/s]<ipython-input-41-5ea830fa7d19>:200: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).cuda()], dim=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class OptimizedResults(Dataset):\n",
        "    def __init__(self, ds, c):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ds: Dataset containing translations\n",
        "            c: Constraints dictionary\n",
        "        \"\"\"\n",
        "        self.data = ds\n",
        "        self.c = c\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def constraints_tokens(self, src):\n",
        "        \"\"\"Efficiently extract constraint tokens\"\"\"\n",
        "        ngrams = get_ngrams(src)\n",
        "        constraints_src = []\n",
        "        seen_grams = set()\n",
        "\n",
        "        for ngram in ngrams:\n",
        "            if ngram in self.c:\n",
        "                for gram in self.c[ngram][0]:\n",
        "                    if gram not in seen_grams:\n",
        "                        seen_grams.add(gram)\n",
        "                        out = tokenizer(gram, return_tensors=\"pt\")\n",
        "                        constraints_src.append(out[\"input_ids\"])\n",
        "\n",
        "        return constraints_src\n",
        "\n",
        "    def optimized_beam_search(self, src, maxlen=25, k=6, constrained_tokens=None):\n",
        "        \"\"\"\n",
        "        Optimized beam search with batched processing and reduced overhead\n",
        "\n",
        "        Args:\n",
        "            src (str): Source text to translate\n",
        "            maxlen (int): Maximum length of generated sequence\n",
        "            k (int): Number of beams\n",
        "            constrained_tokens (list, optional): List of constraint tokens\n",
        "\n",
        "        Returns:\n",
        "            Best translation candidates\n",
        "        \"\"\"\n",
        "        # Move model to device once\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        # Prepare initial inputs\n",
        "        start_token = model.config.decoder_start_token_id\n",
        "        encoder_inputs = tokenizer(src, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Initialize beam search state\n",
        "        batch_size = 1\n",
        "        num_constraints = len(constrained_tokens) if constrained_tokens else 0\n",
        "\n",
        "        # Initialize beam candidates\n",
        "        sequences = torch.full((batch_size, k, 1), start_token, dtype=torch.long, device=self.device)\n",
        "        sequence_scores = torch.zeros(batch_size, k, device=self.device)\n",
        "\n",
        "        # Track completed sequences\n",
        "        completed_sequences = []\n",
        "        completed_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step in range(maxlen):\n",
        "                # Prepare current decoder inputs for all beams\n",
        "                current_sequences = sequences.view(batch_size * k, -1)\n",
        "\n",
        "                # Generate next tokens for all beams simultaneously\n",
        "                outputs = model(\n",
        "                    input_ids=encoder_inputs.input_ids.repeat(k, 1),\n",
        "                    attention_mask=encoder_inputs.attention_mask.repeat(k, 1),\n",
        "                    decoder_input_ids=current_sequences\n",
        "                )\n",
        "\n",
        "                # Process logits\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                # Combine previous sequence scores with new log probabilities\n",
        "                next_sequence_scores = (\n",
        "                    sequence_scores.unsqueeze(-1) +\n",
        "                    log_probs.view(batch_size, k, -1)\n",
        "                )\n",
        "\n",
        "                # Flatten and find top-k candidates\n",
        "                next_sequence_scores = next_sequence_scores.view(batch_size, -1)\n",
        "                topk_scores, topk_indices = torch.topk(next_sequence_scores, k, dim=-1)\n",
        "\n",
        "                # Reconstruct beam states\n",
        "                next_sequences = []\n",
        "                next_scores = []\n",
        "\n",
        "                for i in range(batch_size):\n",
        "                    # Compute beam and token indices\n",
        "                    beam_indices = topk_indices[i] // logits.shape[-1]\n",
        "                    token_indices = topk_indices[i] % logits.shape[-1]\n",
        "\n",
        "                    # Gather top-k sequences and their scores\n",
        "                    batch_sequences = torch.stack([\n",
        "                        torch.cat([sequences[i, beam_idx], token_indices[j].unsqueeze(0)])\n",
        "                        for j, beam_idx in enumerate(beam_indices)\n",
        "                    ])\n",
        "                    batch_scores = topk_scores[i]\n",
        "\n",
        "                    next_sequences.append(batch_sequences)\n",
        "                    next_scores.append(batch_scores)\n",
        "\n",
        "                # Update sequences and scores\n",
        "                sequences = torch.stack(next_sequences)\n",
        "                sequence_scores = torch.stack(next_scores)\n",
        "\n",
        "                # Optional: Apply constraints if available\n",
        "                # (This is a placeholder and would need specific implementation)\n",
        "\n",
        "                # Check for end of sequence\n",
        "                eos_mask = (sequences[0, :, -1] == tokenizer.eos_token_id)\n",
        "                if eos_mask.any():\n",
        "                    completed_sequences.extend(sequences[0, eos_mask])\n",
        "                    completed_scores.extend(sequence_scores[0, eos_mask])\n",
        "\n",
        "                    # Remove completed sequences from active beams\n",
        "                    active_mask = ~eos_mask\n",
        "                    sequences = sequences[:, active_mask]\n",
        "                    sequence_scores = sequence_scores[:, active_mask]\n",
        "\n",
        "                # Early stopping if no active beams\n",
        "                if len(sequences[0]) == 0:\n",
        "                    break\n",
        "\n",
        "            # Return best translation\n",
        "            if completed_sequences:\n",
        "                best_idx = torch.argmax(torch.tensor(completed_scores))\n",
        "                return completed_sequences[best_idx]\n",
        "            else:\n",
        "                return sequences[0, 0]  # Best current sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.data[\"train\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a single sample and process it with optimized beam search.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (source text, predicted translation, reference translation)\n",
        "        \"\"\"\n",
        "        sample = self.data[\"train\"][idx][\"translation\"][\"tr\"]\n",
        "        constraints = self.constraints_tokens(sample)\n",
        "\n",
        "        # Optimized beam search\n",
        "        prediction = self.optimized_beam_search(\n",
        "            src=sample,\n",
        "            maxlen=25,\n",
        "            k=6,\n",
        "            constrained_tokens=constraints\n",
        "        )\n",
        "\n",
        "        # Decode the prediction\n",
        "        decoded_prediction = tokenizer.decode(prediction, skip_special_tokens=True)\n",
        "        result = self.data[\"train\"][idx][\"translation\"][\"en\"]\n",
        "\n",
        "        return sample, decoded_prediction, result"
      ],
      "metadata": {
        "id": "PJ-SMKrp5dlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate BLEU score\n",
        "# Note: corpus_bleu expects references to be a list of list of lists\n",
        "# (multiple references per prediction) and predictions to be list of lists\n",
        "bleu_score = corpus_bleu(references, [[pred.split()] for pred in predictions])\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "\n",
        "# Additional analysis\n",
        "print(f\"Total Predictions: {len(predictions)}\")\n",
        "print(f\"Total References: {len(references)}\")\n",
        "\n",
        "# Optional: Print a few samples if you want to inspect the data\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(min(5, len(predictions))):\n",
        "    print(f\"Prediction {i+1}: {predictions[i]}\")\n",
        "    print(f\"Reference {i+1}: {references[i][0]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "4RT4CGQh1S2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tr_text))\n",
        "print(len(ds[\"train\"]))"
      ],
      "metadata": {
        "id": "9G3KHuuzwPzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cs = constraints_tokens(ds[\"train\"][1][\"translation\"][\"tr\"], c)\n",
        "print(cs)\n",
        "beam_search(maxlen= 50, numC=0, k =6, src = ds[\"train\"][1][\"translation\"][\"tr\"], constraints = cs)"
      ],
      "metadata": {
        "id": "nBha8-UJMIIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds[\"train\"][1][\"translation\"][\"tr\"])\n",
        "ds[\"train\"][1][\"translation\"][\"en\"]"
      ],
      "metadata": {
        "id": "ihW0o7aR_Xt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"Kosova, tekrar eden şikayetler ışığında özelleştirme sürecini incelemeye alıyor.\"\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Generate translation\n",
        "translated_tokens = model.generate(**inputs)\n",
        "\n",
        "# Decode and print the translation\n",
        "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "print(\"Translated text:\", translated_text)"
      ],
      "metadata": {
        "id": "Ig3j9O3hWcsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9uAIYL3iX1Wh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvikasreddy/lexically_constrained_beam_search_/blob/main/beam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "Marian MT model : https://huggingface.co/docs/transformers/model_doc/marian\n",
        "\n",
        "Code to get the logits : https://huggingface.co/docs/transformers/main_classes/output\n",
        "\n",
        "to get the BOS and EOS tokens: https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig.decoder_start_token_id\n",
        "\n",
        "get topk values : https://pytorch.org/docs/stable/generated/torch.topk.html\n",
        "\n",
        "ideas and core implementation drawn from this paper: https://arxiv.org/pdf/1704.07138\n",
        "\n",
        "reference to link google colab with .py file from git : https://colab.research.google.com/github/jckantor/cbe61622/blob/master/docs/A.02-Downloading_Python_source_files_from_github.ipynb\n"
      ],
      "metadata": {
        "id": "irykjFilkb0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading essential modules"
      ],
      "metadata": {
        "id": "4FVX-zdj1zoV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTeRXGcN1aLG",
        "outputId": "0e8bf920-1307-4834-a61c-3132e3e2d2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "39v0-NQ5jW6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "BsIWSdWT1nXy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the dataset, considering the wmt turkish - english translation"
      ],
      "metadata": {
        "id": "4ryAXcurjhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"wmt/wmt16\", \"tr-en\")"
      ],
      "metadata": {
        "id": "wh8kntBaDQkQ",
        "outputId": "3f5f683f-a95c-4670-b1e4-09c0298093fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glancing the organization of the dataset"
      ],
      "metadata": {
        "id": "xUKJaLMBjwHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZmmV6aM18Zm",
        "outputId": "5c606001-6d49-47c4-e6dc-4f731d31e818"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 205756\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1001\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VskMyKZb7W3U",
        "outputId": "6859b0e5-95bf-43f3-930b-a610662cdc9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': \"Kosovo's privatisation process is under scrutiny\",\n",
              "  'tr': \"Kosova'nın özelleştirme süreci büyüteç altında\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the tokenizer and model, based of Marian-NMT"
      ],
      "metadata": {
        "id": "xKqvu8eej9_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-D25X8-3fDH",
        "outputId": "707ec8ab-9135-48e4-c6ca-5c9d679f4ec3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq8hERj4pMq",
        "outputId": "125a4535-6b29-4aec-b9ff-84fc428fe06d",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(62389, 512, padding_idx=62388)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLU()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLU()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=62389, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_translation(src_text, max_length = 50):\n",
        "\n",
        "#   # Tokenize input\n",
        "#   encoder_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
        "\n",
        "#   # intializes the decoder input with decoder start token\n",
        "#   decoder_input = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "#   # chang the model to eval mode.\n",
        "#   model.eval()\n",
        "#   with torch.no_grad():\n",
        "\n",
        "#     generated_tokens = []\n",
        "\n",
        "#     while len(generated_tokens) < max_length:\n",
        "\n",
        "#       outputs = model(\n",
        "#           input_ids=encoder_inputs.input_ids,\n",
        "#           attention_mask=encoder_inputs.attention_mask,\n",
        "#           decoder_input_ids=decoder_input\n",
        "#       )\n",
        "\n",
        "\n",
        "#       next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "\n",
        "#       # get the token with maximum logits value\n",
        "#       next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "#       generated_tokens.append(next_token.item())\n",
        "\n",
        "#       if next_token.item() == tokenizer.eos_token_id:\n",
        "#           break\n",
        "#       print(decoder_input)\n",
        "#       decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0)], dim=1)\n",
        "#       print(decoder_input)\n",
        "#     translated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "#   return translated_text\n"
      ],
      "metadata": {
        "id": "chKWkv56DJNT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### step by Step generation\n",
        "# for i in range(len(generated_tokens)):\n",
        "#     partial_text = tokenizer.decode(generated_tokens[:i+1], skip_special_tokens=True)\n",
        "#     print(f\"Step {i+1}: {partial_text}\")"
      ],
      "metadata": {
        "id": "AVfheO9c8MM3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.num_beams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kypz225xV07O",
        "outputId": "a7cd4860-decb-42ef-d320-a3c9302df719"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(generate_translation(ds['validation'][1]['translation']['tr']))"
      ],
      "metadata": {
        "id": "XUR-FySHjfHZ",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"validation\"][1][\"translation\"][\"tr\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ld36ZcknI28I",
        "outputId": "6dc62338-e5c5-45fc-932e-634a1e7c423d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Norveç'in beş milyon insanı en yüksek yaşam standartlarının tadını çıkarıyor, sadece Avrupa'da değil, dünyada.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"validation\"][1][\"translation\"][\"en\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "2qleapnBJH_O",
        "outputId": "e63d3b92-f321-424f-e21e-177939e75e2b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Norway's five million people enjoy one of the highest standards of living, not just in Europe, but in the world.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting the constraints"
      ],
      "metadata": {
        "id": "716cOsrattrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to import constraints and store in a local directory, from my git\n",
        "\n",
        "user = \"vvikasreddy\"\n",
        "repo = \"lexically_constrained_beam_search_\"\n",
        "pyfile = \"constraints.py\"\n",
        "\n",
        "# i.e url is \"https://github.com/vvikasreddy/lexically_constrained_beam_search_/blob/main/constraints.py\"\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{pyfile}\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "\n",
        "import constraints"
      ],
      "metadata": {
        "id": "jrClS2MolMXW",
        "collapsed": true,
        "outputId": "de9aa1d4-7dcd-4f0e-a6b3-ffe5cc9d6aad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-04 20:17:24--  https://raw.githubusercontent.com/vvikasreddy/lexically_constrained_beam_search_/main/constraints.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4914 (4.8K) [text/plain]\n",
            "Saving to: ‘constraints.py’\n",
            "\n",
            "\rconstraints.py        0%[                    ]       0  --.-KB/s               \rconstraints.py      100%[===================>]   4.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-04 20:17:24 (52.2 MB/s) - ‘constraints.py’ saved [4914/4914]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# takes almost 4 minutes to get the constraints, you will see 3 progress bars\n",
        "c = constraints.get_constraints()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L9LeDVKbZlK1",
        "outputId": "4ce03691-0ee5-41e4-98b5-0334c065eddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205756/205756 [00:57<00:00, 3585.56it/s]\n",
            "100%|██████████| 205756/205756 [01:39<00:00, 2062.50it/s]\n",
            "100%|██████████| 26221852/26221852 [00:58<00:00, 448485.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"some of the constraints are :\")\n",
        "\n",
        "# Extract 5 random keys\n",
        "random_keys = random.sample(list(c.keys()), 5)\n",
        "\n",
        "for key in random_keys:\n",
        "  print(key, c[key])\n",
        "\n",
        "print(\"The length of the constraints is\", len(c))"
      ],
      "metadata": {
        "id": "z-aHWm6_h8wu",
        "outputId": "f498ab10-bbdb-46d6-c3a6-9f30e8b0fddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "some of the constraints are :\n",
            "('ABD', 'Dışişleri') (('of', 'State'), 1.1697696008746603)\n",
            "('eski', 'Bosnalı') (('former', 'Bosnian'), 1.0410855671492016)\n",
            "('Orta', 've') (('Central', 'and'), 0.993313184724903)\n",
            "('Başbakanı', 'Vojislav') (('Serbian', 'Prime'), 1.0148018894754143)\n",
            "(\"Jovanoviç'in\", 'haberi') (('Jovanovic', 'for'), 1.1927609036257918)\n",
            "The length of the constraints is 570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_translation(src_text, decoder_input = [], probabilites = [], get_constrained_token_probability = -1, k = 5):\n",
        "\n",
        "  \"\"\"\n",
        "    returns decoder_input_tokens, probs, vis_data\n",
        "\n",
        "    decoder_input_tokens : next top k tokens, or constraints probability if get_constrained_token_probabliity != -1\n",
        "    probs : corresponding probablities of decoder_input_tokens\n",
        "    vis_data : top k beams\n",
        "\n",
        "    generate the decoder input ids and corresponding probabilities\n",
        "    src_text : It is the source text\n",
        "    decoder_input : Represents the decoder tokens\n",
        "    probabilities : Represents corresponding decoder token probablities\n",
        "    get_constrained_token_probabliity : holds the constraint, -1 indicates no constraint,\n",
        "    k : number of beams to be generated, default is 5\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  decoder_input_tokens = []\n",
        "  probs = []\n",
        "  vis_data = []\n",
        "\n",
        "  # Tokenize input\n",
        "  encoder_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
        "\n",
        "  # if decoder_input is empty, then include the decoder start token\n",
        "  if decoder_input == []:\n",
        "    # intial decoder start token has probability 1\n",
        "    probabilites = torch.tensor([[1]])\n",
        "    decoder_input = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "\n",
        "  # change the model to eval mode and stop the computation of gradients.\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids=encoder_inputs.input_ids,\n",
        "        attention_mask=encoder_inputs.attention_mask,\n",
        "        decoder_input_ids=decoder_input\n",
        "    )\n",
        "\n",
        "    # gets the most frequenlty generated token.\n",
        "    next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    # constraint, if provided, returns the probability.\n",
        "    if get_constrained_token_probability != -1:\n",
        "      softmax_  = torch.softmax(next_token_logits, dim=-1)\n",
        "      return softmax_[0][get_constrained_token_probability]\n",
        "\n",
        "    # get the top k tokens with maximum logits value\n",
        "    top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k = k)\n",
        "\n",
        "  for indx, id in enumerate(top_indices[0]):\n",
        "    decoder_input_tokens.append(torch.cat([decoder_input, id.unsqueeze(0).unsqueeze(0)], dim=1))\n",
        "    probs.append(torch.cat([probabilites, top_probs[0][indx].unsqueeze(0).unsqueeze(0)], dim=1))\n",
        "    vis_data.append((vis_data, tokenizer.decode(decoder_input_tokens[indx].squeeze(), skip_special_tokens = True)))\n",
        "\n",
        "  return decoder_input_tokens, probs, vis_data\n",
        "\n",
        "x,y,z = generate_translation(ds['validation'][1]['translation']['tr'], decoder_input = torch.tensor([[62388,  1969]]), probabilites = torch.tensor([[0.0000, 0.0000]]))\n",
        "print( x)\n",
        "print(y)\n",
        "print(z)\n",
        "# c"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Lb1AIoB-akSu",
        "outputId": "42888583-aae8-44af-e3eb-2869e78a525d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[62388,  1969,   261]]), tensor([[62388,  1969,    15]]), tensor([[62388,  1969,   510]]), tensor([[62388,  1969,     8]]), tensor([[62388,  1969,    47]])]\n",
            "[tensor([[0.0000, 0.0000, 0.3950]]), tensor([[0.0000, 0.0000, 0.1113]]), tensor([[0.0000, 0.0000, 0.0247]]), tensor([[0.0000, 0.0000, 0.0110]]), tensor([[0.0000, 0.0000, 0.0105]])]\n",
            "[([...], 'Koso'), ([...], 'Kost'), ([...], 'Kosum'), ([...], 'Koss'), ([...], 'Kosa')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_ngrams(src, n = 2, ):\n",
        "\n",
        "#   src = src.split(\" \")\n",
        "#   src = [tuple(src[i:i+n]) for i in range(len(src) - n + 1)]\n",
        "\n",
        "#   return src\n",
        "\n",
        "# def constraints_tokens(src, c):\n",
        "#   ngrams = get_ngrams(src)\n",
        "#   constraints_src = []\n",
        "#   for ngram in ngrams:\n",
        "#     # print(ngram)\n",
        "#     if ngram in c:\n",
        "#       for gram in ngram:\n",
        "\n",
        "#         constraints_src.append(tokenizer(gram, return_tensors=\"pt\"))\n",
        "#   return constraints_src\n",
        "\n",
        "# def get_input_ids(data):\n",
        "\n",
        "#   input_ids = []\n",
        "#   for example in data:\n",
        "#     input_ids.append((example['input_ids'][0].tolist())[0])\n",
        "#   return input_ids"
      ],
      "metadata": {
        "id": "m31ZnR46R1S0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = constraints_tokens(ds['validation'][1]['translation']['tr'] + ' Times' +  ' için', c)\n",
        "# # c\n",
        "# print(x)\n",
        "# print(get_input_ids(x))"
      ],
      "metadata": {
        "id": "YAE_CMBYSeHr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(decoder_input):\n",
        "  return tokenizer.decode(decoder_input.squeeze(), skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "ecRWZg9IDyQl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_top_k_prob(A, B, k=2):\n",
        "\n",
        "  d = {}\n",
        "  # cummulative sum\n",
        "  for indx, val in enumerate(B):\n",
        "    print(val, type(val))\n",
        "    cum_sum = torch.prod(val)\n",
        "    d[cum_sum] = indx\n",
        "\n",
        "  sorted_keys = sorted(d.keys(), reverse = True)\n",
        "\n",
        "  top_k_indices = []\n",
        "  top_k_sequences = []\n",
        "\n",
        "  for key in sorted_keys[:k]:\n",
        "    top_k_indices.append(A[d[key]])\n",
        "    top_k_sequences.append(B[d[key]])\n",
        "\n",
        "  return top_k_sequences, top_k_indices\n",
        "\n",
        "# sanity\n",
        "k = 2\n",
        "\n",
        "# A = [torch.tensor([[62388,   626,    13]]), torch.tensor([[62388,   626,     9]]), torch.tensor([[62388,   626,  1341]]), torch.tensor([[62388,   626,    27]])]\n",
        "# B = [torch.tensor([[1.0000, 0.0038, 0.2500]]), torch.tensor([[1.0000, 0.0038, 0.0619]]), torch.tensor([[1.0000, 0.0038, 0.0474]]), torch.tensor([[1.0000, 0.0038, 0.0425]])]\n",
        "\n",
        "\n",
        "A = [torch.tensor([[62388,  1969]]), torch.tensor([[62388,   323]]), torch.tensor([[62388,    67]]), torch.tensor([[62388,  1132]]), torch.tensor([[62388,   626]])]\n",
        "B = [torch.tensor([[1.0000, 0.8746]]), torch.tensor([[1.0000, 0.0156]]), torch.tensor([[1.0000, 0.0114]]), torch.tensor([[1.0000, 0.0039]]), torch.tensor([[1.0000, 0.0038]])]\n",
        "\n",
        "top_sequences, indices = get_top_k_prob(A, B, k)\n",
        "print(f\"Top {k} sequences:\", top_sequences)\n",
        "print(\"Their indices:\", indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGCXp6x4OEj2",
        "outputId": "8ebcb905-9a50-4873-ea21-a317271b5255"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.8746]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0156]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0114]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0039]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0038]]) <class 'torch.Tensor'>\n",
            "Top 2 sequences: [tensor([[1.0000, 0.8746]]), tensor([[1.0000, 0.0156]])]\n",
            "Their indices: [tensor([[62388,  1969]]), tensor([[62388,   323]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(maxlen, numC, k, src, constraints):\n",
        "\n",
        "    decoder_start_token = model.config.decoder_start_token_id\n",
        "\n",
        "    # initialize the grids\n",
        "    grids = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "    probs_grid  = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "\n",
        "    # intialize the first grid to start hyp\n",
        "    grids[0][0] = [1]\n",
        "\n",
        "    # remove during testsrc\n",
        "    # constrained_tokens = get_input_ids(constraints_tokens(, constraints))\n",
        "    # temporary\n",
        "    constrained_tokens = [3762, 37]\n",
        "\n",
        "    generated_constraint_index = 0\n",
        "\n",
        "    for t in range(1, maxlen):\n",
        "\n",
        "      index_c = max(0, (numC - t) - maxlen)\n",
        "\n",
        "      g = torch.tensor([])\n",
        "      s = torch.tensor([])\n",
        "      probs = torch.tensor([])\n",
        "\n",
        "      for c in range(index_c, min(t, numC) + 1):\n",
        "\n",
        "          print(\"cur iteration \", t, c )\n",
        "          s = []\n",
        "          g = []\n",
        "\n",
        "          # storing decoder inputs\n",
        "          decoder_inputs = []\n",
        "          probs = []\n",
        "          vis_data = []\n",
        "\n",
        "          # print(grids)\n",
        "          print(probs_grid[t-1][c], \"hey yo \", t - 1, c, \"-------------------\")\n",
        "          for indx, element in enumerate(grids[t-1][c]):\n",
        "\n",
        "            # guess there is no need for conditioning, just generate.\n",
        "            print(element, \"this is the element\", )\n",
        "            if type(element) == int:\n",
        "              decoder_input = []\n",
        "              probs =[]\n",
        "            else:\n",
        "              decoder_input = element\n",
        "              probs = probs_grid[t-1][c][indx]\n",
        "\n",
        "            # print(element)\n",
        "            # print(decoder_input)\n",
        "            # print(probs)\n",
        "            # print(\"----------------------------\")\n",
        "            t_g, t_probs,vis_data = generate_translation(src_text= src, decoder_input = decoder_input, probabilites = probs)\n",
        "            print(type(g))\n",
        "            g.append(t_g)\n",
        "            print(probs)\n",
        "            print(type(probs))\n",
        "            probs.append(t_probs)\n",
        "            # g += t_g\n",
        "            # probs += t_probs\n",
        "            print(probs)\n",
        "            print(vis_data)\n",
        "            print(\"here\")\n",
        "\n",
        "          # retrieve the  probability of the constraint and add that to the decoder_input.\n",
        "          if c > 0 and constrained_tokens:\n",
        "\n",
        "            for indx, element in enumerate(grids[t-1][c-1]):\n",
        "\n",
        "              if c == 1 and t == 1:\n",
        "                decoder_inputs = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "                prob = torch.tensor([[1]])\n",
        "              else:\n",
        "                decoder_inputs = element\n",
        "                prob = probs_grid[t-1][c-1][indx]\n",
        "\n",
        "              # Gets the constraints probability and stores them\n",
        "              cons = generate_translation(src, decoder_input = decoder_input, get_constrained_token_probability = constrained_tokens[c - 1])\n",
        "              # print(cons, decoder_inputs, element)\n",
        "              decoder_inputs = torch.cat([decoder_inputs, torch.tensor(constrained_tokens[c-1]).unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "              prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "\n",
        "\n",
        "              g.append(decoder_inputs)\n",
        "\n",
        "              probs.append(prob)\n",
        "          print(\"before\")\n",
        "          print(g, \"g\")\n",
        "          print(probs, \"probs\")\n",
        "          probs_grid[t][c], grids[t][c] = get_top_k_prob(g, probs, k)\n",
        "\n",
        "\n",
        "          # print(grids[t][c], t, c )\n",
        "          # print(probs_grid[t][c])\n",
        "          for i in  grids[t][c]:\n",
        "            print(visualize_data(i), i )\n",
        "\n",
        "\n",
        "          print('asd-------------------------------------------')\n",
        "\n",
        "  # sanity : print grids\n",
        "    print(grids)\n",
        "\n",
        "\n",
        "beam_search(maxlen= 5, numC=0, k =6, src = ds[\"train\"][1][\"translation\"][\"tr\"], constraints = c)"
      ],
      "metadata": {
        "id": "rp4JV4xqay72",
        "outputId": "ee25394e-264c-4e53-afe3-298047aae61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cur iteration  1 0\n",
            "[] hey yo  0 0 -------------------\n",
            "1 this is the element\n",
            "<class 'list'>\n",
            "[]\n",
            "<class 'list'>\n",
            "[[tensor([[1.0000, 0.8746]]), tensor([[1.0000, 0.0156]]), tensor([[1.0000, 0.0114]]), tensor([[1.0000, 0.0039]]), tensor([[1.0000, 0.0038]])]]\n",
            "[([...], 'Kos'), ([...], 'In'), ([...], 'He'), ([...], 'With'), ([...], 'As')]\n",
            "here\n",
            "before\n",
            "[[tensor([[62388,  1969]]), tensor([[62388,   323]]), tensor([[62388,    67]]), tensor([[62388,  1132]]), tensor([[62388,   626]])]] g\n",
            "[[tensor([[1.0000, 0.8746]]), tensor([[1.0000, 0.0156]]), tensor([[1.0000, 0.0114]]), tensor([[1.0000, 0.0039]]), tensor([[1.0000, 0.0038]])]] probs\n",
            "tensor([[1.0000, 0.8746]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0156]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0114]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0039]]) <class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.0038]]) <class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-6719a49f7083>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-6719a49f7083>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(maxlen, numC, k, src, constraints)\u001b[0m\n\u001b[1;32m     90\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m           \u001b[0mprobs_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_k_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-f8276d23d78e>\u001b[0m in \u001b[0;36mget_top_k_prob\u001b[0;34m(A, B, k)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtop_k_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtop_k_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(maxlen, numC, k, src, constraints):\n",
        "\n",
        "    decoder_start_token = model.config.decoder_start_token_id\n",
        "\n",
        "    # initialize the grids\n",
        "    grids = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "    probs_grid  = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "\n",
        "    # intialize the first grid to start hyp\n",
        "    grids[0][0] = [1]\n",
        "\n",
        "    # remove during testsrc\n",
        "    # constrained_tokens = get_input_ids(constraints_tokens(, constraints))\n",
        "    # temporary\n",
        "    constrained_tokens = [3762, 37]\n",
        "\n",
        "    generated_constraint_index = 0\n",
        "\n",
        "    for t in range(1, maxlen):\n",
        "\n",
        "      index_c = max(0, (numC - t) - maxlen)\n",
        "\n",
        "      g = torch.tensor([])\n",
        "      s = torch.tensor([])\n",
        "      probs = torch.tensor([])\n",
        "\n",
        "      for c in range(index_c, min(t, numC) + 1):\n",
        "\n",
        "          print(\"cur iteration \", t, c )\n",
        "          s = []\n",
        "          g = []\n",
        "\n",
        "          # storing decoder inputs\n",
        "          decoder_inputs = []\n",
        "          probs = []\n",
        "          vis_data = []\n",
        "\n",
        "          # print(grids)\n",
        "          print(probs_grid[t-1][c], \"hey yo \", t - 1, c, \"-------------------\")\n",
        "          for indx, element in enumerate(grids[t-1][c]):\n",
        "\n",
        "            # guess there is no need for conditioning, just generate.\n",
        "            print(element, \"this is the element\", )\n",
        "            if type(element) == int:\n",
        "              decoder_input = []\n",
        "              probs =[]\n",
        "            else:\n",
        "              decoder_input = element\n",
        "              probs = probs_grid[t-1][c][indx]\n",
        "\n",
        "            # print(element)\n",
        "            # print(decoder_input)\n",
        "            # print(probs)\n",
        "            # print(\"----------------------------\")\n",
        "            g, probs,vis_data = generate_translation(src_text= src, decoder_input = decoder_input, probabilites = probs)\n",
        "            # print(type(g))\n",
        "            # g.append(t_g)\n",
        "            # print(probs)\n",
        "            # print(type(probs))\n",
        "            # probs.append(t_probs)\n",
        "            # g += t_g\n",
        "            # probs += t_probs\n",
        "            print(probs)\n",
        "            print(vis_data)\n",
        "            print(\"here\")\n",
        "\n",
        "          # retrieve the  probability of the constraint and add that to the decoder_input.\n",
        "          if c > 0 and constrained_tokens:\n",
        "\n",
        "            for indx, element in enumerate(grids[t-1][c-1]):\n",
        "\n",
        "              if c == 1 and t == 1:\n",
        "                decoder_inputs = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "                prob = torch.tensor([[1]])\n",
        "              else:\n",
        "                decoder_inputs = element\n",
        "                prob = probs_grid[t-1][c-1][indx]\n",
        "\n",
        "              # Gets the constraints probability and stores them\n",
        "              cons = generate_translation(src, decoder_input = decoder_input, get_constrained_token_probability = constrained_tokens[c - 1])\n",
        "              # print(cons, decoder_inputs, element)\n",
        "              decoder_inputs = torch.cat([decoder_inputs, torch.tensor(constrained_tokens[c-1]).unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "              prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "\n",
        "\n",
        "              g.append(decoder_inputs)\n",
        "\n",
        "              probs.append(prob)\n",
        "          print(\"before\")\n",
        "          print(g, \"g\")\n",
        "          print(probs, \"probs\")\n",
        "          probs_grid[t][c], grids[t][c] = get_top_k_prob(g, probs, k)\n",
        "\n",
        "\n",
        "          # print(grids[t][c], t, c )\n",
        "          # print(probs_grid[t][c])\n",
        "          for i in  grids[t][c]:\n",
        "            print(visualize_data(i), i )\n",
        "\n",
        "\n",
        "          print('asd-------------------------------------------')\n",
        "\n",
        "  # sanity : print grids\n",
        "    print(grids)\n",
        "\n",
        "\n",
        "beam_search(maxlen= 5, numC=0, k =6, src = ds[\"train\"][1][\"translation\"][\"tr\"], constraints = c)"
      ],
      "metadata": {
        "id": "MczBbs89pi3G",
        "outputId": "6c89d7ec-85bd-4c16-fe1e-050f3a4f4b93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cur iteration  1 0\n",
            "[] hey yo  0 0 -------------------\n",
            "1 this is the element\n",
            "[tensor([[1.0000, 0.8746]]), tensor([[1.0000, 0.0156]]), tensor([[1.0000, 0.0114]]), tensor([[1.0000, 0.0039]]), tensor([[1.0000, 0.0038]])]\n",
            "[([...], 'Kos'), ([...], 'In'), ([...], 'He'), ([...], 'With'), ([...], 'As')]\n",
            "here\n",
            "before\n",
            "[tensor([[62388,  1969]]), tensor([[62388,   323]]), tensor([[62388,    67]]), tensor([[62388,  1132]]), tensor([[62388,   626]])] g\n",
            "[tensor([[1.0000, 0.8746]]), tensor([[1.0000, 0.0156]]), tensor([[1.0000, 0.0114]]), tensor([[1.0000, 0.0039]]), tensor([[1.0000, 0.0038]])] probs\n",
            "Kos tensor([[62388,  1969]])\n",
            "In tensor([[62388,   323]])\n",
            "He tensor([[62388,    67]])\n",
            "With tensor([[62388,  1132]])\n",
            "As tensor([[62388,   626]])\n",
            "asd-------------------------------------------\n",
            "cur iteration  2 0\n",
            "[tensor([[1.0000, 0.8746]]), tensor([[1.0000, 0.0156]]), tensor([[1.0000, 0.0114]]), tensor([[1.0000, 0.0039]]), tensor([[1.0000, 0.0038]])] hey yo  1 0 -------------------\n",
            "tensor([[62388,  1969]]) this is the element\n",
            "[tensor([[1.0000, 0.8746, 0.9285]]), tensor([[1.0000, 0.8746, 0.0019]]), tensor([[1.0000, 0.8746, 0.0014]]), tensor([[1.0000e+00, 8.7463e-01, 5.9243e-04]]), tensor([[1.0000e+00, 8.7463e-01, 2.9697e-04]])]\n",
            "[([...], 'Koso'), ([...], 'Kosova'), ([...], 'Kosa'), ([...], 'Kosia'), ([...], 'Kosov')]\n",
            "here\n",
            "tensor([[62388,   323]]) this is the element\n",
            "[tensor([[1.0000, 0.0156, 0.9235]]), tensor([[1.0000, 0.0156, 0.0340]]), tensor([[1.0000, 0.0156, 0.0134]]), tensor([[1.0000, 0.0156, 0.0034]]), tensor([[1.0000, 0.0156, 0.0025]])]\n",
            "[([...], 'In light'), ([...], 'In view'), ([...], 'In the'), ([...], 'In response'), ([...], 'In accordance')]\n",
            "here\n",
            "tensor([[62388,    67]]) this is the element\n",
            "[tensor([[1.0000, 0.0114, 0.4241]]), tensor([[1.0000, 0.0114, 0.0590]]), tensor([[1.0000, 0.0114, 0.0460]]), tensor([[1.0000, 0.0114, 0.0373]]), tensor([[1.0000, 0.0114, 0.0177]])]\n",
            "[([...], 'He is'), ([...], 'He examine'), ([...], 'He has'), ([...], \"He'\"), ([...], 'He investigate')]\n",
            "here\n",
            "tensor([[62388,  1132]]) this is the element\n",
            "[tensor([[1.0000, 0.0039, 0.7913]]), tensor([[1.0000, 0.0039, 0.0461]]), tensor([[1.0000, 0.0039, 0.0204]]), tensor([[1.0000, 0.0039, 0.0090]]), tensor([[1.0000, 0.0039, 0.0055]])]\n",
            "[([...], 'With repeated'), ([...], 'With the'), ([...], 'With a'), ([...], 'With repeat'), ([...], 'With rep')]\n",
            "here\n",
            "tensor([[62388,   626]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.2500]]), tensor([[1.0000, 0.0038, 0.0619]]), tensor([[1.0000, 0.0038, 0.0474]]), tensor([[1.0000, 0.0038, 0.0425]]), tensor([[1.0000, 0.0038, 0.0319]])]\n",
            "[([...], 'As a'), ([...], 'As the'), ([...], 'As light'), ([...], 'As for'), ([...], 'As regards')]\n",
            "here\n",
            "before\n",
            "[tensor([[62388,   626,    13]]), tensor([[62388,   626,     9]]), tensor([[62388,   626,  1341]]), tensor([[62388,   626,    27]]), tensor([[62388,   626, 23658]])] g\n",
            "[tensor([[1.0000, 0.0038, 0.2500]]), tensor([[1.0000, 0.0038, 0.0619]]), tensor([[1.0000, 0.0038, 0.0474]]), tensor([[1.0000, 0.0038, 0.0425]]), tensor([[1.0000, 0.0038, 0.0319]])] probs\n",
            "As a tensor([[62388,   626,    13]])\n",
            "As the tensor([[62388,   626,     9]])\n",
            "As light tensor([[62388,   626,  1341]])\n",
            "As for tensor([[62388,   626,    27]])\n",
            "As regards tensor([[62388,   626, 23658]])\n",
            "asd-------------------------------------------\n",
            "cur iteration  3 0\n",
            "[tensor([[1.0000, 0.0038, 0.2500]]), tensor([[1.0000, 0.0038, 0.0619]]), tensor([[1.0000, 0.0038, 0.0474]]), tensor([[1.0000, 0.0038, 0.0425]]), tensor([[1.0000, 0.0038, 0.0319]])] hey yo  2 0 -------------------\n",
            "tensor([[62388,   626,    13]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.2500, 0.6489]]), tensor([[1.0000, 0.0038, 0.2500, 0.0632]]), tensor([[1.0000, 0.0038, 0.2500, 0.0429]]), tensor([[1.0000, 0.0038, 0.2500, 0.0118]]), tensor([[1.0000, 0.0038, 0.2500, 0.0116]])]\n",
            "[([...], 'As a result'), ([...], 'As a light'), ([...], 'As a consequence'), ([...], 'As a view'), ([...], 'As a matter')]\n",
            "here\n",
            "tensor([[62388,   626,     9]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0619, 0.0378]]), tensor([[1.0000, 0.0038, 0.0619, 0.0293]]), tensor([[1.0000, 0.0038, 0.0619, 0.0280]]), tensor([[1.0000, 0.0038, 0.0619, 0.0157]]), tensor([[1.0000, 0.0038, 0.0619, 0.0156]])]\n",
            "[([...], 'As the country'), ([...], 'As the results'), ([...], 'As the complaint'), ([...], 'As the report'), ([...], 'As the complaints')]\n",
            "here\n",
            "tensor([[62388,   626,  1341]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0474, 0.8666]]), tensor([[1.0000, 0.0038, 0.0474, 0.0047]]), tensor([[1.0000, 0.0038, 0.0474, 0.0043]]), tensor([[1.0000, 0.0038, 0.0474, 0.0027]]), tensor([[1.0000, 0.0038, 0.0474, 0.0019]])]\n",
            "[([...], 'As light of'), ([...], 'As light for'), ([...], 'As light on'), ([...], 'As light as'), ([...], 'As lighting')]\n",
            "here\n",
            "tensor([[62388,   626,    27]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0425, 0.3353]]), tensor([[1.0000, 0.0038, 0.0425, 0.2333]]), tensor([[1.0000, 0.0038, 0.0425, 0.0229]]), tensor([[1.0000, 0.0038, 0.0425, 0.0198]]), tensor([[1.0000, 0.0038, 0.0425, 0.0158]])]\n",
            "[([...], 'As for the'), ([...], 'As for repeated'), ([...], 'As for Kos'), ([...], 'As for re'), ([...], 'As for a')]\n",
            "here\n",
            "tensor([[62388,   626, 23658]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.8109]]), tensor([[1.0000, 0.0038, 0.0319, 0.0587]]), tensor([[1.0000, 0.0038, 0.0319, 0.0136]]), tensor([[1.0000, 0.0038, 0.0319, 0.0074]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069]])]\n",
            "[([...], 'As regards repeated'), ([...], 'As regards the'), ([...], 'As regards rep'), ([...], 'As regards re'), ([...], 'As regards repeat')]\n",
            "here\n",
            "before\n",
            "[tensor([[62388,   626, 23658, 31187]]), tensor([[62388,   626, 23658,     9]]), tensor([[62388,   626, 23658, 16635]]), tensor([[62388,   626, 23658,   934]]), tensor([[62388,   626, 23658,  8422]])] g\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.8109]]), tensor([[1.0000, 0.0038, 0.0319, 0.0587]]), tensor([[1.0000, 0.0038, 0.0319, 0.0136]]), tensor([[1.0000, 0.0038, 0.0319, 0.0074]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069]])] probs\n",
            "As regards repeated tensor([[62388,   626, 23658, 31187]])\n",
            "As regards the tensor([[62388,   626, 23658,     9]])\n",
            "As regards rep tensor([[62388,   626, 23658, 16635]])\n",
            "As regards re tensor([[62388,   626, 23658,   934]])\n",
            "As regards repeat tensor([[62388,   626, 23658,  8422]])\n",
            "asd-------------------------------------------\n",
            "cur iteration  4 0\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.8109]]), tensor([[1.0000, 0.0038, 0.0319, 0.0587]]), tensor([[1.0000, 0.0038, 0.0319, 0.0136]]), tensor([[1.0000, 0.0038, 0.0319, 0.0074]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069]])] hey yo  3 0 -------------------\n",
            "tensor([[62388,   626, 23658, 31187]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.8109, 0.8364]]), tensor([[1.0000, 0.0038, 0.0319, 0.8109, 0.0415]]), tensor([[1.0000, 0.0038, 0.0319, 0.8109, 0.0414]]), tensor([[1.0000, 0.0038, 0.0319, 0.8109, 0.0083]]), tensor([[1.0000, 0.0038, 0.0319, 0.8109, 0.0046]])]\n",
            "[([...], 'As regards repeated complaints'), ([...], 'As regards repeated grievance'), ([...], 'As regards repeated complaint'), ([...], 'As regards repeated complaining'), ([...], 'As regards repeated complain')]\n",
            "here\n",
            "tensor([[62388,   626, 23658,     9]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.0587, 0.8001]]), tensor([[1.0000, 0.0038, 0.0319, 0.0587, 0.0193]]), tensor([[1.0000, 0.0038, 0.0319, 0.0587, 0.0167]]), tensor([[1.0000, 0.0038, 0.0319, 0.0587, 0.0084]]), tensor([[1.0000, 0.0038, 0.0319, 0.0587, 0.0070]])]\n",
            "[([...], 'As regards the repeated'), ([...], 'As regards the rep'), ([...], 'As regards the recurring'), ([...], 'As regards the re'), ([...], 'As regards the repeating')]\n",
            "here\n",
            "tensor([[62388,   626, 23658, 16635]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.0136, 0.9100]]), tensor([[1.0000, 0.0038, 0.0319, 0.0136, 0.0099]]), tensor([[1.0000, 0.0038, 0.0319, 0.0136, 0.0045]]), tensor([[1.0000, 0.0038, 0.0319, 0.0136, 0.0038]]), tensor([[1.0000, 0.0038, 0.0319, 0.0136, 0.0037]])]\n",
            "[([...], 'As regards repeti'), ([...], 'As regards reprod'), ([...], 'As regards repub'), ([...], 'As regards repris'), ([...], 'As regards repeal')]\n",
            "here\n",
            "tensor([[62388,   626, 23658,   934]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.0074, 0.4321]]), tensor([[1.0000, 0.0038, 0.0319, 0.0074, 0.0821]]), tensor([[1.0000, 0.0038, 0.0319, 0.0074, 0.0495]]), tensor([[1.0000, 0.0038, 0.0319, 0.0074, 0.0409]]), tensor([[1.0000, 0.0038, 0.0319, 0.0074, 0.0400]])]\n",
            "[([...], 'As regards recur'), ([...], 'As regards res'), ([...], 'As regards re-'), ([...], 'As regards ret'), ([...], 'As regards repet')]\n",
            "here\n",
            "tensor([[62388,   626, 23658,  8422]]) this is the element\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.8502]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0305]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0277]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0076]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0054]])]\n",
            "[([...], 'As regards repeat complaints'), ([...], 'As regards repeat complaint'), ([...], 'As regards repeat grievance'), ([...], 'As regards repeats'), ([...], 'As regards repeat complaining')]\n",
            "here\n",
            "before\n",
            "[tensor([[62388,   626, 23658,  8422, 25899]]), tensor([[62388,   626, 23658,  8422, 18442]]), tensor([[62388,   626, 23658,  8422, 48888]]), tensor([[62388,   626, 23658,  8422,     8]]), tensor([[62388,   626, 23658,  8422, 18867]])] g\n",
            "[tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.8502]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0305]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0277]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0076]]), tensor([[1.0000, 0.0038, 0.0319, 0.0069, 0.0054]])] probs\n",
            "As regards repeat complaints tensor([[62388,   626, 23658,  8422, 25899]])\n",
            "As regards repeat complaint tensor([[62388,   626, 23658,  8422, 18442]])\n",
            "As regards repeat grievance tensor([[62388,   626, 23658,  8422, 48888]])\n",
            "As regards repeats tensor([[62388,   626, 23658,  8422,     8]])\n",
            "As regards repeat complaining tensor([[62388,   626, 23658,  8422, 18867]])\n",
            "asd-------------------------------------------\n",
            "[[[1]], [[tensor([[62388,  1969]]), tensor([[62388,   323]]), tensor([[62388,    67]]), tensor([[62388,  1132]]), tensor([[62388,   626]])]], [[tensor([[62388,   626,    13]]), tensor([[62388,   626,     9]]), tensor([[62388,   626,  1341]]), tensor([[62388,   626,    27]]), tensor([[62388,   626, 23658]])]], [[tensor([[62388,   626, 23658, 31187]]), tensor([[62388,   626, 23658,     9]]), tensor([[62388,   626, 23658, 16635]]), tensor([[62388,   626, 23658,   934]]), tensor([[62388,   626, 23658,  8422]])]], [[tensor([[62388,   626, 23658,  8422, 25899]]), tensor([[62388,   626, 23658,  8422, 18442]]), tensor([[62388,   626, 23658,  8422, 48888]]), tensor([[62388,   626, 23658,  8422,     8]]), tensor([[62388,   626, 23658,  8422, 18867]])]], [[]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds[\"train\"][1][\"translation\"][\"tr\"])\n",
        "ds[\"train\"][1][\"translation\"][\"en\"]"
      ],
      "metadata": {
        "id": "ihW0o7aR_Xt_",
        "outputId": "bca38bdb-0948-47e2-d16c-9d0ac83fafe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kosova, tekrar eden şikayetler ışığında özelleştirme sürecini incelemeye alıyor.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kosovo is taking a hard look at its privatisation process in light of recurring complaints.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"Kosova, tekrar eden şikayetler ışığında özelleştirme sürecini incelemeye alıyor.\"\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Generate translation\n",
        "translated_tokens = model.generate(**inputs)\n",
        "\n",
        "# Decode and print the translation\n",
        "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "print(\"Translated text:\", translated_text)"
      ],
      "metadata": {
        "id": "Ig3j9O3hWcsV",
        "outputId": "f3585102-c212-41af-eb2d-94e7281562c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text: Kosovo is reviewing the privatisation process in light of repeated complaints.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9uAIYL3iX1Wh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
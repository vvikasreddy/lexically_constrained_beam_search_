{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvikasreddy/lexically_constrained_beam_search_/blob/main/beam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "Marian MT model : https://huggingface.co/docs/transformers/model_doc/marian\n",
        "\n",
        "Code to get the logits : https://huggingface.co/docs/transformers/main_classes/output\n",
        "\n",
        "to get the BOS and EOS tokens: https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig.decoder_start_token_id\n",
        "\n",
        "get topk values : https://pytorch.org/docs/stable/generated/torch.topk.html\n",
        "\n",
        "ideas and core implementation drawn from this paper: https://arxiv.org/pdf/1704.07138\n",
        "\n",
        "bleu score : https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
        "\n",
        "rogue score : https://huggingface.co/spaces/evaluate-metric/rouge/blob/main/README.md\n",
        "\n",
        "(not directly related to beam search project)\n",
        "\n",
        "\n",
        "reference to link google colab with .py file from git  : https://colab.research.google.com/github/jckantor/cbe61622/blob/master/docs/A.02-Downloading_Python_source_files_from_github.ipynb\n",
        "\n",
        "reference to get rogue score working : https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "\n"
      ],
      "metadata": {
        "id": "irykjFilkb0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading essential modules"
      ],
      "metadata": {
        "id": "4FVX-zdj1zoV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTeRXGcN1aLG",
        "outputId": "772b6769-39a7-4876-b578-04e441075fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "39v0-NQ5jW6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "BsIWSdWT1nXy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the dataset, considering the wmt turkish - english translation"
      ],
      "metadata": {
        "id": "4ryAXcurjhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"wmt/wmt16\", \"tr-en\")"
      ],
      "metadata": {
        "id": "wh8kntBaDQkQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glancing the organization of the dataset"
      ],
      "metadata": {
        "id": "xUKJaLMBjwHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZmmV6aM18Zm",
        "outputId": "06055f8f-c897-4d89-f672-2a68ab7911a5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 205756\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1001\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VskMyKZb7W3U",
        "outputId": "66e66bcb-d7e6-45d5-9a89-70b1e59772f5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': \"Kosovo's privatisation process is under scrutiny\",\n",
              "  'tr': \"Kosova'nın özelleştirme süreci büyüteç altında\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the tokenizer and model, based of Marian-NMT"
      ],
      "metadata": {
        "id": "xKqvu8eej9_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tr-en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-D25X8-3fDH",
        "outputId": "46cc7138-46b9-4474-8ce3-4286b58c0d8b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq8hERj4pMq",
        "outputId": "193021d4-299d-477e-9c39-213227d1dfde",
        "collapsed": true
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(62389, 512, padding_idx=62388)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLU()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(62389, 512, padding_idx=62388)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLU()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=62389, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a look at the number of beams used by the model.\n",
        "model.config.num_beams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kypz225xV07O",
        "outputId": "684630d6-9765-44c7-8ae6-5a0476d228d7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing the turkish and english translation\n",
        "print(ds[\"validation\"][1][\"translation\"][\"tr\"])\n",
        "print(ds[\"validation\"][1][\"translation\"][\"en\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld36ZcknI28I",
        "outputId": "7df3ec5c-4a96-474d-bf3f-40a7c034d259"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norveç'in beş milyon insanı en yüksek yaşam standartlarının tadını çıkarıyor, sadece Avrupa'da değil, dünyada.\n",
            "Norway's five million people enjoy one of the highest standards of living, not just in Europe, but in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting and importing the constraints from my github"
      ],
      "metadata": {
        "id": "716cOsrattrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to import constraints and store in a local directory, from my git. (code reference cicted above.)\n",
        "\n",
        "user = \"vvikasreddy\"\n",
        "repo = \"lexically_constrained_beam_search_\"\n",
        "pyfile = \"constraints.py\"\n",
        "\n",
        "# i.e url is \"https://github.com/vvikasreddy/lexically_constrained_beam_search_/blob/main/constraints.py\"\n",
        "\n",
        "url = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{pyfile}\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "\n",
        "import constraints"
      ],
      "metadata": {
        "id": "jrClS2MolMXW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c1c5f8-c99a-4709-ef24-f1b65591259f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 20:01:38--  https://raw.githubusercontent.com/vvikasreddy/lexically_constrained_beam_search_/main/constraints.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4914 (4.8K) [text/plain]\n",
            "Saving to: ‘constraints.py’\n",
            "\n",
            "constraints.py      100%[===================>]   4.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-06 20:01:38 (65.7 MB/s) - ‘constraints.py’ saved [4914/4914]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# takes almost 4 minutes to get the constraints, you will see 3 progress bars\n",
        "c = constraints.get_constraints()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L9LeDVKbZlK1",
        "outputId": "1808f9ed-a791-4683-b872-d7d5f15b83d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205756/205756 [00:44<00:00, 4590.91it/s]\n",
            "100%|██████████| 205756/205756 [01:02<00:00, 3305.78it/s]\n",
            "100%|██████████| 26221852/26221852 [00:40<00:00, 651646.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"some of the constraints are :\")\n",
        "\n",
        "# Extract 5 random keys\n",
        "random_keys = random.sample(list(c.keys()), 5)\n",
        "\n",
        "for key in random_keys:\n",
        "  print(key, c[key])\n",
        "\n",
        "print(\"The length of the constraints is\", len(c))"
      ],
      "metadata": {
        "id": "z-aHWm6_h8wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fe68f6-4b67-42b4-bc81-85eb6a288cb1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "some of the constraints are :\n",
            "('Sekreteri', 'Jaap') (('Jaap', 'de'), 1.0258203699570452)\n",
            "('BM', 'elçisi') (('UN', 'envoy'), 0.918902866483925)\n",
            "('ve', 'güvenlik') (('and', 'security'), 0.9645569334807078)\n",
            "('Sekreteri', 'Ban') (('Secretary-General', 'Ban'), 0.9598333810538963)\n",
            "('Ratko', 'Mladiç') (('Ratko', 'Mladic'), 0.9253616486062107)\n",
            "The length of the constraints is 570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions for Beam Search"
      ],
      "metadata": {
        "id": "iCaHlsGX2vQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(src, n = 2, ):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    src: text for which ngrams should be returned\n",
        "    n : represents the value of ngrams\n",
        "\n",
        "  returns:\n",
        "    returns ngrams\n",
        "  \"\"\"\n",
        "\n",
        "  src = src.split(\" \")\n",
        "  src = [tuple(src[i:i+n]) for i in range(len(src) - n + 1)]\n",
        "  return src\n",
        "\n",
        "def constraints_tokens(src, c):\n",
        "  \"\"\"\n",
        "  args:\n",
        "    src: It is the turkish sentence, to which we want to return the constraints\n",
        "    c: represents the entire list of constraints\n",
        "\n",
        "  returns:\n",
        "    returns the corresponding constraints of the src text\n",
        "  \"\"\"\n",
        "\n",
        "  # gets the ngrams\n",
        "  ngrams = get_ngrams(src)\n",
        "\n",
        "  constraints_src = []\n",
        "  for ngram in ngrams:\n",
        "\n",
        "    # if ngram is present then add it to the constraints list\n",
        "    if ngram in c:\n",
        "      f = c[ngram][0]\n",
        "      for gram in f:\n",
        "        if  gram in constraints_src: continue\n",
        "        out = tokenizer(gram, return_tensors=\"pt\")\n",
        "        constraints_src.append(out[\"input_ids\"])\n",
        "\n",
        "  return constraints_src\n",
        "\n",
        "# this are some of the example constraints of the sample text in turkish\n",
        "constraints_tokens(\"Southeast European Times için Priştine'den Muhamet Brayşori'nin haberi -- 21/03/12\", c)"
      ],
      "metadata": {
        "id": "m31ZnR46R1S0",
        "outputId": "4999cd72-7d8a-4f77-a4fc-1625493da2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[3113,   56,   47, 1517,    0]]),\n",
              " tensor([[5827, 1786,  373,    0]]),\n",
              " tensor([[5827, 1786,  373,    0]]),\n",
              " tensor([[3762,    0]]),\n",
              " tensor([[27,  0]]),\n",
              " tensor([[3113,   56,   47, 1517,    0]]),\n",
              " tensor([[21,  0]]),\n",
              " tensor([[ 4388, 10158,   204,     0]]),\n",
              " tensor([[1417,    0]]),\n",
              " tensor([[6041,   47, 2628,    0]]),\n",
              " tensor([[27,  0]]),\n",
              " tensor([[3113,   56,   47, 1517,    0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(decoder_input):\n",
        "  \"\"\"\n",
        "  code used for sanity, to visualize the constraints.\n",
        "  args:\n",
        "    decoder_input : takes in decoder token ids\n",
        "  returns:\n",
        "    returns the tokens\"\"\"\n",
        "\n",
        "  return tokenizer.decode(decoder_input.squeeze(), skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "ecRWZg9IDyQl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_indices(ds, c):\n",
        "  \"\"\"\n",
        "  Since wmt dataset is not domain speicific, I am trying to only take the sentences, for which constraints are present.\n",
        "\n",
        "  args:\n",
        "    ds: represents the hugging face WMT dataset\n",
        "    c: represents the constraints\n",
        "  returns:\n",
        "    indices: which contains the first 1000 indices, that have constraints present.\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  indices = []\n",
        "  count = 750\n",
        "  i = 0\n",
        "\n",
        "  while count:\n",
        "    x = ds[\"train\"][i][\"translation\"][\"tr\"]\n",
        "    if constraints_tokens(x, c):\n",
        "      count -=1\n",
        "      indices.append(i)\n",
        "    i+=1\n",
        "\n",
        "  return indices\n",
        "\n",
        "print(\"The last index in all of the 3000 is,\", get_indices(ds,c)[-1])"
      ],
      "metadata": {
        "id": "dYfl26o1HxrL",
        "outputId": "39a2be8a-9aff-431c-f163-bf66874c4de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The last index in all of the 3000 is, 1594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Results(Dataset):\n",
        "\n",
        "  \"\"\"\n",
        "  Results class is the implementation of the Beam search\n",
        "\n",
        "  Methods:\n",
        "\n",
        "  generate_translation: Generates the next best token\n",
        "  get_the_text: returns the translated text at the end of beam search\n",
        "  get_top_k_prob: given a bunch of translations, it returns the top k beams\n",
        "  constraints_tokens: returns the constraints of the particular target sentence (i.e in our case turkish sentence)\n",
        "  beam_search: tries to return the best translation, we have a cap of maxlength 30, and beamsize of 6\n",
        "  __len__: returns the length of indices considered (in our case 1000, same as the size of validation set)\n",
        "  __getitem__: for each item called returns the translated sentence at the end of beam search\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, ds, c, indices, device = None):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          ds : represents the Hugging face dataset\n",
        "          c: represents all the constraints generated\n",
        "          indices: these are the indices which we consider for the test\n",
        "          device: chooses the device based on the configs\n",
        "      \"\"\"\n",
        "      self.data = ds\n",
        "      self.c = c\n",
        "      self.indices = indices\n",
        "      if device is None:\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def generate_translation(self, src_text, decoder_input=[], probabilities=[], get_constrained_token_probability=-1, k=6):\n",
        "\n",
        "    \"\"\"\n",
        "    this method generates the k best translations and their probabilites\n",
        "\n",
        "    args:\n",
        "      src_text : target sentence in turkish\n",
        "      decoder_inputs : holds the token ids in english, up till that timestep\n",
        "      probabliities : holds the corresponding probabilities of the decoder_inputs\n",
        "      get_constrained_token_probablity: holds value -1, if there is no constraint, else contains the constraint token id\n",
        "      k : represents the beam size, which is set to 6\n",
        "\n",
        "    returns:\n",
        "      decoder_input_ids: it holds the decoder input ids so far\n",
        "      probabilities: holds the corresponding probabilities of the decoder inputs\n",
        "      vis_data: used for sanity purposes, to visualize the generated data\n",
        "      \"\"\"\n",
        "\n",
        "    # holds cuda if available else cpu\n",
        "    device = self.device\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize input\n",
        "    encoder_inputs = tokenizer(src_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # If decoder_input is empty, include the decoder start token\n",
        "    if len(decoder_input) == 0:\n",
        "      # Initial decoder start token has probability 1\n",
        "      probabilities = torch.tensor([[1.0]]).to(device)\n",
        "      decoder_input = torch.tensor([[model.config.decoder_start_token_id]]).to(device)\n",
        "    else:\n",
        "      # Ensure decoder_input and probabilities are on the correct device\n",
        "      decoder_input = decoder_input.to(device)\n",
        "      if probabilities != [] :\n",
        "        probabilities = probabilities.to(device)\n",
        "\n",
        "    # Change the model to eval mode and stop the computation of gradients\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      # Generate tokens\n",
        "      outputs = model(\n",
        "          input_ids=encoder_inputs.input_ids,\n",
        "          attention_mask=encoder_inputs.attention_mask,\n",
        "          decoder_input_ids=decoder_input\n",
        "      )\n",
        "\n",
        "      # Get the most frequently generated token\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "      # Constraint handling\n",
        "      if get_constrained_token_probability != -1:\n",
        "          softmax_ = torch.softmax(next_token_logits, dim=-1)\n",
        "          return softmax_[0][get_constrained_token_probability]\n",
        "\n",
        "      # Get the top k tokens with maximum logits value\n",
        "      top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k=k)\n",
        "\n",
        "    # Initializing output containers\n",
        "    decoder_input_tokens = []\n",
        "    probs = []\n",
        "    vis_data = []\n",
        "\n",
        "    for indx, id in enumerate(top_indices[0]):\n",
        "        # Concatenate new tokens and probabilities\n",
        "        new_decoder_input = torch.cat([decoder_input, id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        new_probs = torch.cat([probabilities, top_probs[0][indx].unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        decoder_input_tokens.append(new_decoder_input)\n",
        "        probs.append(new_probs)\n",
        "\n",
        "        # Generate visualization data, used for sanity (removed for submission)\n",
        "        vis_data.append((vis_data, tokenizer.decode(new_decoder_input.squeeze(), skip_special_tokens=True)))\n",
        "\n",
        "    return decoder_input_tokens, probs, vis_data\n",
        "\n",
        "  def get_the_text(self, x):\n",
        "    \"\"\"\n",
        "    Given decoder token ids, it converts to tokens\n",
        "\n",
        "    args:\n",
        "      x: represents the decoder token ids\n",
        "    returns:\n",
        "      the corresponding translated sentences in english\n",
        "    \"\"\"\n",
        "\n",
        "    token_id_list = x\n",
        "\n",
        "    # moves tensors to cpu\n",
        "    token_ids_batch = [tensor_item.cpu().squeeze().tolist() for tensor_item in token_id_list]\n",
        "\n",
        "    decoded_sentences = tokenizer.batch_decode(token_ids_batch, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_sentences\n",
        "\n",
        "  def get_top_k_prob(self, A, B, k=6):\n",
        "\n",
        "    \"\"\"\n",
        "    takes in decoder inputs, and corresponding probablities and returns the k best beams\n",
        "\n",
        "    args:\n",
        "      A: represents the list of decoder input ids\n",
        "      B: represents the list of correspondding probabilities\n",
        "      k : it is the beam size\n",
        "    returns:\n",
        "      returns the k best values.\"\"\"\n",
        "\n",
        "    # container to hold the cummulative probablility sum of the beams\n",
        "    d = {}\n",
        "\n",
        "    # cummulative sum\n",
        "    for indx, val in enumerate(B):\n",
        "      cum_sum = torch.prod(val)\n",
        "      d[cum_sum] = indx\n",
        "\n",
        "    # sort to the cummulative probability sum\n",
        "    sorted_keys = sorted(d.keys(), reverse = True)\n",
        "\n",
        "    # containers to store top k values\n",
        "    top_k_indices = []\n",
        "    top_k_sequences = []\n",
        "\n",
        "    for key in sorted_keys[:k]:\n",
        "      top_k_indices.append(A[d[key]])\n",
        "      top_k_sequences.append(B[d[key]])\n",
        "\n",
        "    return top_k_sequences, top_k_indices\n",
        "\n",
        "  def constraints_tokens(self, src):\n",
        "    \"\"\"\n",
        "    args:\n",
        "      src: It is the turkish sentence, to which we want to return the constraints\n",
        "      c: represents the entire list of constraints\n",
        "\n",
        "    returns:\n",
        "      returns the corresponding constraints of the src text\n",
        "  \"\"\"\n",
        "    # gets the ngrams\n",
        "    ngrams = get_ngrams(src)\n",
        "    constraints_src = []\n",
        "\n",
        "    for ngram in ngrams:\n",
        "      # if ngram is present then add it to the constraints list\n",
        "      if ngram in self.c:\n",
        "        f = self.c[ngram][0]\n",
        "        for gram in f:\n",
        "\n",
        "          if  gram in constraints_src: continue\n",
        "          out = tokenizer(gram, return_tensors=\"pt\")\n",
        "          constraints_src.append(out[\"input_ids\"])\n",
        "\n",
        "    return constraints_src\n",
        "\n",
        "  def beam_search(self, maxlen, numC, k, src, constrained_tokens):\n",
        "\n",
        "    \"\"\"\n",
        "    Combines all the methods to return the k best beams\n",
        "\n",
        "    args:\n",
        "      maxlen: it is the max length, until where we generate sentences\n",
        "      numC: represents the number of constraints\n",
        "      k: represents the number of beams\n",
        "      src: represents the target sentences, in turkish language\n",
        "      constrained_tokens: represnts the constraints associated with the corresponding sentence\n",
        "\n",
        "    returns:\n",
        "      returns 2 sentences,\n",
        "      1. sentence containing the constraints associated with the sentence\n",
        "      2. sentence generated without any constraints\n",
        "    \"\"\"\n",
        "\n",
        "    # device where values are present\n",
        "    device = self.device\n",
        "\n",
        "    decoder_start_token = model.config.decoder_start_token_id\n",
        "\n",
        "    # initialize the grids\n",
        "    grids = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "    probs_grid  = [[[] for _ in range(numC + 1)] for _ in range(maxlen + 1)]\n",
        "\n",
        "    # intialize the first grid to start hyp\n",
        "    grids[0][0] = [1]\n",
        "\n",
        "    generated_constraint_index = 0\n",
        "\n",
        "    # iterate through the timestep\n",
        "    for t in range(1, maxlen):\n",
        "\n",
        "        index_c = max(0, (numC - t) - maxlen)\n",
        "        # iterate through constraints\n",
        "        for c in range(index_c, min(t, numC) + 1):\n",
        "\n",
        "            # Prepare batched generation to reduce individual calls, g holds the decoder input tokens, for the current timestep.\n",
        "            g = []\n",
        "\n",
        "            # storing decoder inputs\n",
        "            decoder_inputs = []\n",
        "            probs = []\n",
        "            vis_data = []\n",
        "\n",
        "            # generation of translations for current hypotheses\n",
        "            for indx, element in enumerate(grids[t-1][c]):\n",
        "\n",
        "              if type(element) == int:\n",
        "                decoder_input = []\n",
        "                prev_probs =[]\n",
        "              else:\n",
        "                decoder_input = element.to(device)\n",
        "                prev_probs = probs_grid[t-1][c][indx].to(device)\n",
        "\n",
        "              # collecting the batch of translations\n",
        "              t_g, t_probs, t_vis_data = self.generate_translation(src_text=src, decoder_input=decoder_input, probabilities=prev_probs)\n",
        "\n",
        "              # adding to the current lists\n",
        "              g.extend(t_g)\n",
        "              probs.extend(t_probs)\n",
        "              vis_data.extend(t_vis_data)\n",
        "\n",
        "            # retrieve the probability of the constraint and add that to the decoder_input.\n",
        "            if c > 0 and constrained_tokens:\n",
        "\n",
        "              for indx, element in enumerate(grids[t-1][c-1]):\n",
        "\n",
        "                if c == 1 and t == 1:\n",
        "                  decoder_inputs = torch.tensor([[model.config.decoder_start_token_id]]).to(device)\n",
        "                  prob = torch.tensor([[1]]).to(device)\n",
        "                else:\n",
        "                  decoder_inputs = element.to(device)\n",
        "                  prob = probs_grid[t-1][c-1][indx].to(device)\n",
        "\n",
        "                # iterating, because a constraint can be made up of many token ids\n",
        "                partial_constraints = constrained_tokens[c - 1].tolist()\n",
        "\n",
        "                # iterating, because a constraint can be made up of many token ids\n",
        "                for partial_constraint in partial_constraints[0]:\n",
        "                  if partial_constraint == 0: continue\n",
        "\n",
        "\n",
        "                  cons = self.generate_translation(src, decoder_input=decoder_input, get_constrained_token_probability=partial_constraint)\n",
        "                  decoder_inputs = torch.cat([decoder_inputs, torch.tensor(partial_constraint).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n",
        "                  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n",
        "\n",
        "                  # appending to generated token ids (g), and probabilites to prob (probs)\n",
        "                  g.append(decoder_inputs)\n",
        "                  probs.append(prob)\n",
        "\n",
        "            # storing the k best token ids to the current grid.\n",
        "            probs_grid[t][c], grids[t][c] = self.get_top_k_prob(g, probs, k)\n",
        "\n",
        "    # return the best token ids which contains all the constraints, and ones without any constraints.\n",
        "    return self.get_top_k_prob(grids[maxlen -1][numC], probs_grid[maxlen - 1][numC], k = 1), self.get_top_k_prob(grids[maxlen -1][0], probs_grid[maxlen - 1][0], k = 1)\n",
        "\n",
        "  def __len__(self):\n",
        "      \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "      return len(self.indices)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Retrieves a single target sentence, and returns prediction with constraints, prediction without any constraints, and reference translation\n",
        "\n",
        "      args:\n",
        "          idx: Index of the sample to retrieve.\n",
        "\n",
        "      Returns:\n",
        "          returns target sentence, prediction with constraints, prediction without any constraints, and reference translation\n",
        "      \"\"\"\n",
        "\n",
        "      sample = self.data[\"train\"][self.indices[idx]][\"translation\"][\"tr\"]\n",
        "\n",
        "      # gets the constraints for that sentence\n",
        "      constraints = self.constraints_tokens(sample)\n",
        "\n",
        "      # using beam search gets the predicted translations, both with and without constraints\n",
        "      prediction, prediction_without_constraints = self.beam_search(maxlen= 23, numC=len(constraints), k = 4, src = sample, constrained_tokens = constraints)\n",
        "\n",
        "      # convert the token ids to tokens\n",
        "      prediction = self.get_the_text(prediction[1])\n",
        "      prediction_without_constraints = self.get_the_text(prediction_without_constraints[1])\n",
        "      result = self.data[\"train\"][self.indices[idx]][\"translation\"][\"en\"]\n",
        "\n",
        "      if len(prediction) > 1: prediction = prediction[0][:]\n",
        "      if len(prediction_without_constraints) > 0 : prediction_without_constraints = prediction_without_constraints[0][:]\n",
        "\n",
        "      return sample, prediction, prediction_without_constraints, result"
      ],
      "metadata": {
        "id": "iZSxRCdiBmA_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the instance of the Results class\n",
        "indices = get_indices(ds,c)\n",
        "results = Results(ds,c, indices = indices)\n",
        "\n",
        "# generating sentences for 43th index\n",
        "results[43]"
      ],
      "metadata": {
        "id": "GkErwbFeFpwO",
        "outputId": "81016991-56f8-4ae6-dd49-ab4a95d61d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-c3409a6ca3c5>:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Sırbistan'ın, Kosova'da yaşayan Sırp nüfusun ve yanı sıra dünya genelinde birçok ülkenin bu eyleme karşı çıkması, ciddi sorunlarla karşı karşıya kalacağımızın habercisi.\",\n",
              " [\"Serbia's opposition to this action bloc, the Serb population in Koso as well as many\"],\n",
              " \"Serbia's opposition to the Serbian population in Kosovo, as well as many countries around the world\",\n",
              " 'The fact that neither Serbia nor the Serbian population in Kosovo as well as a number of countries throughout the world agree with such an act speaks of a period of serious challenges we are facing.')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store predictions and actual texts\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  predictions = []\n",
        "  predictions_without_constraints = []\n",
        "  references = []\n",
        "\n",
        "  # using a dataloader, to get the batches of 10\n",
        "  dataloader = DataLoader(results, batch_size=10, shuffle=False)\n",
        "\n",
        "  # Iterate through dataloader and storing results in containers\n",
        "  for indx, (tr_text, pred_text, pred_cons_text, actual_text) in tqdm(enumerate(dataloader)):\n",
        "    predictions.extend([ref.split() for ref in pred_text])\n",
        "    predictions_without_constraints.extend([ref.split() for ref in pred_cons_text])\n",
        "    references.extend([[ref.split()] for ref in actual_text])\n",
        ""
      ],
      "metadata": {
        "id": "feByO35xG0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "e6d84b16-f8fb-4693-9833-6d5eb6983d12"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]<ipython-input-58-c31efe201fab>:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prob = torch.cat([prob, torch.tensor(cons).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n",
            "1it [04:33, 273.08s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-cc7aa203a6ef>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Iterate through dataloader and storing results in containers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mindx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtr_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cons_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-c31efe201fab>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0;31m# using beam search gets the predicted translations, both with and without constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m       \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_without_constraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstrained_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0;31m# convert the token ids to tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-c31efe201fab>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, maxlen, numC, k, src, constrained_tokens)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m               \u001b[0;31m# collecting the batch of translations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m               \u001b[0mt_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_vis_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m               \u001b[0;31m# adding to the current lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-c31efe201fab>\u001b[0m in \u001b[0;36mgenerate_translation\u001b[0;34m(self, src_text, decoder_input, probabilities, get_constrained_token_probability, k)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Change the model to eval mode and stop the computation of gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;31m# Generate tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2863\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m         \"\"\"\n\u001b[0;32m-> 2865\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2844\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2845\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m                 raise AttributeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             if isinstance(instance, torch.Tensor) and getattr(\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_param\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             ):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "\n",
        "def BLEU_score(references, translations):\n",
        "  \"\"\"\n",
        "  Averages the BLEU score over all the translations\n",
        "\n",
        "  args:\n",
        "    references: represents acutal translations\n",
        "    translations: represents predicted translations\n",
        "  returns:\n",
        "    average BLEU score\n",
        "  \"\"\"\n",
        "  bleu_scores = []\n",
        "\n",
        "  # since my constraints were generated only for ngrams of size 2\n",
        "  weights = [0.5, 0.5,0,0]\n",
        "\n",
        "  for ref, trans in zip(references, translations):\n",
        "    # Calculate BLEU score for this sentence\n",
        "    score = sentence_bleu(ref, trans, weights = weights)\n",
        "    bleu_scores.append(score)\n",
        "\n",
        "  # Calculate and return average BLEU score\n",
        "  return sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "\n",
        "bleu_score_constraints = BLEU_score(references, predictions)\n",
        "bleu_score_without_constraints = BLEU_score(references, predictions_without_constraints)\n",
        "print(f\"BLEU Score with constraints: {bleu_score_constraints:.4f}, BLEU score without constraints: {bleu_score_without_constraints:.4f}\")"
      ],
      "metadata": {
        "id": "qTg5A2VhHGLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "  return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# reference to get this working : https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working"
      ],
      "metadata": {
        "id": "9G3KHuuzwPzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "Tx9sbK52V5Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def calculate_rouge_with_evaluate(references, translations):\n",
        "    \"\"\"\n",
        "    Calculate average ROUGE-1, ROUGE-2, and ROUGE-L F1 scores using the `evaluate` library.\n",
        "\n",
        "    args:\n",
        "      references: represents acutal translations\n",
        "      translations: represents predicted translations\n",
        "    returns:\n",
        "      average BLEU score\n",
        "  \"\"\"\n",
        "    # Load the ROUGE metric\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "    # joining the references and translations into one list\n",
        "    references_flat = [\" \".join(ref[0]) for ref in references]\n",
        "    translations_flat = [\" \".join(trans) for trans in translations]\n",
        "    results = rouge.compute(predictions=translations_flat, references=references_flat)\n",
        "\n",
        "    return {\n",
        "        \"rouge_1\": results[\"rouge1\"],\n",
        "        \"rouge_2\": results[\"rouge2\"],\n",
        "        \"rouge_L\": results[\"rougeL\"]\n",
        "    }\n",
        "\n",
        "# Calculate ROUGE scores for predictions with and without constraints\n",
        "rouge_scores_constraints = calculate_rouge_with_evaluate(references, predictions)\n",
        "rouge_scores_no_constraints = calculate_rouge_with_evaluate(references, predictions_without_constraints)\n",
        "\n",
        "print(f\"ROUGE-1 with constraints: {rouge_scores_constraints['rouge_1']:.4f}\")\n",
        "print(f\"ROUGE-2 with constraints: {rouge_scores_constraints['rouge_2']:.4f}\")\n",
        "print(f\"ROUGE-L with constraints: {rouge_scores_constraints['rouge_L']:.4f}\")\n",
        "\n",
        "print(f\"ROUGE-1 without constraints: {rouge_scores_no_constraints['rouge_1']:.4f}\")\n",
        "print(f\"ROUGE-2 without constraints: {rouge_scores_no_constraints['rouge_2']:.4f}\")\n",
        "print(f\"ROUGE-L without constraints: {rouge_scores_no_constraints['rouge_L']:.4f}\")\n"
      ],
      "metadata": {
        "id": "n3xXKVeCPjY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eaD2_g7BV3jt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}